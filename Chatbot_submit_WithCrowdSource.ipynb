{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d67aee85-f8c9-4281-b9a0-5e9ada39e953",
      "metadata": {
        "id": "d67aee85-f8c9-4281-b9a0-5e9ada39e953"
      },
      "source": [
        "### Import the knowledge graph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/Speakeasy_Project')"
      ],
      "metadata": {
        "id": "qVhRFOTaxuMs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec40ac23-90bd-4fcd-ee61-9b97ea25b27f"
      },
      "id": "qVhRFOTaxuMs",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4443faac-5648-49c6-bdd3-896595ccd507",
      "metadata": {
        "id": "4443faac-5648-49c6-bdd3-896595ccd507",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ab277dd-063a-4b03-ea3d-7f039f6e5406"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rdflib in /usr/local/lib/python3.10/dist-packages (7.1.1)\n",
            "Requirement already satisfied: isodate<1.0.0,>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from rdflib) (0.7.2)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from rdflib) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install rdflib # Install the rdflib package\n",
        "from rdflib.term import URIRef, Literal\n",
        "import rdflib\n",
        "import torch\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4271440a-1527-49de-b840-246f0feeb235",
      "metadata": {
        "id": "4271440a-1527-49de-b840-246f0feeb235",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86411b3a-0b34-443a-c5a8-dce153d8895e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Graph identifier=Ne354a34c582b45a7a7cef9eb25451fc5 (<class 'rdflib.graph.Graph'>)>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "graph = rdflib.Graph()\n",
        "graph.parse('14_graph.nt', format='turtle')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5518924f-0463-4cb7-ae40-54a2de8cbce6",
      "metadata": {
        "id": "5518924f-0463-4cb7-ae40-54a2de8cbce6"
      },
      "source": [
        "### NameSpaces\n",
        "\n",
        "The entities are stored with different URIs. The most common namespaces are the following:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d3863b05-3984-4087-8edf-ad588fe73672",
      "metadata": {
        "id": "d3863b05-3984-4087-8edf-ad588fe73672"
      },
      "outputs": [],
      "source": [
        "# define some prefixes\n",
        "WD = rdflib.Namespace('http://www.wikidata.org/entity/')\n",
        "WDT = rdflib.Namespace('http://www.wikidata.org/prop/direct/')\n",
        "DDIS = rdflib.Namespace('http://ddis.ch/atai/')\n",
        "RDFS = rdflib.namespace.RDFS\n",
        "SCHEMA = rdflib.Namespace('http://schema.org/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8e5c6022-462a-4729-a5ee-0db67861b59c",
      "metadata": {
        "scrolled": true,
        "id": "8e5c6022-462a-4729-a5ee-0db67861b59c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e44c348c-a243-4162-90eb-2bb7680bda9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Some subjects from the knowledge graph\n",
            "http://www.wikidata.org/entity/Q352159\n",
            "http://www.wikidata.org/entity/Q1126430\n",
            "http://www.wikidata.org/entity/Q901649\n",
            "http://www.wikidata.org/entity/Q5284135\n",
            "http://www.wikidata.org/entity/Q2033052\n",
            "http://www.wikidata.org/entity/Q366301\n",
            "http://www.wikidata.org/entity/Q57923356\n",
            "http://www.wikidata.org/entity/Q2749247\n",
            "http://www.wikidata.org/entity/Q4965049\n",
            "http://www.wikidata.org/entity/Q7436332\n",
            "\n",
            " Some objects from the knowledge graph\n",
            "2013 American computer-animated adventure comedy film\n",
            "Anneke Blok\n",
            "museum specializing in the display of objects relating to ships and travel on large bodies of water\n",
            "nm0347066\n",
            "Tiffany Mulheron\n",
            "Ben Uttley\n",
            "Grimsby\n",
            "nm1324324\n",
            "nm0001013\n",
            "nm1240978\n"
          ]
        }
      ],
      "source": [
        "print('Some subjects from the knowledge graph')\n",
        "for objs in list(set(graph.subjects()))[:10]:\n",
        "    print(objs)\n",
        "\n",
        "print('\\n Some objects from the knowledge graph')\n",
        "for objs in list(set(graph.objects()))[10:20]:\n",
        "    print(objs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f12dba1-7ff1-43b8-a66b-b131ed4ca1a5",
      "metadata": {
        "id": "0f12dba1-7ff1-43b8-a66b-b131ed4ca1a5"
      },
      "source": [
        "Some ways to access the label of an entity in the graph subjects given it's URI:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "029c242a-d30f-4255-922e-3a6d90647c54",
      "metadata": {
        "id": "029c242a-d30f-4255-922e-3a6d90647c54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff14ebad-3099-46ee-ce32-0af20346fb2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "node http://www.wikidata.org/entity/Q1018481 has label Bye Bye Love\n"
          ]
        }
      ],
      "source": [
        "for node in graph.subjects():\n",
        "    if graph.value(subject=node, predicate=RDFS.label): # Check if the triple exists\n",
        "        print(f\"node {node} has label {graph.value(subject=node, predicate=RDFS.label)}\")\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fc0c3e0-a3c3-4ed2-ac18-724fadaf089a",
      "metadata": {
        "id": "5fc0c3e0-a3c3-4ed2-ac18-724fadaf089a"
      },
      "source": [
        "We want to check if every subject in the graph has a label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0f5f0fdd-f91f-40db-98b5-765e04c8ff7f",
      "metadata": {
        "id": "0f5f0fdd-f91f-40db-98b5-765e04c8ff7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff51498a-f6fc-430b-d791-39a6857c8a4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of subjects with a label: 2051387\n",
            "\n",
            "Number of subjects in the graph: 2056777\n",
            "\n",
            "There are 5390 subject entities without a label\n"
          ]
        }
      ],
      "source": [
        "i = 0\n",
        "j = 0\n",
        "for node in graph.subjects():\n",
        "    j += 1\n",
        "    if graph.value(subject=node, predicate=RDFS.label): # Check if the triple exists\n",
        "        i += 1\n",
        "\n",
        "print(f\"Number of subjects with a label: {i}\\n\")\n",
        "print(f\"Number of subjects in the graph: {j}\\n\")\n",
        "if i != j:\n",
        "    print(f\"There are {j-i} subject entities without a label\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eed26093-bd80-46a4-a55e-4cb5caa5abbe",
      "metadata": {
        "id": "eed26093-bd80-46a4-a55e-4cb5caa5abbe"
      },
      "source": [
        "### Make a dictionary of nodes URIs with the respective labels\n",
        "\n",
        "We want to make a dictionary in which the keys are the nodes URIs and the values are the nodes labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0aa69a55-b3df-4127-a32a-aaf4396d1ae6",
      "metadata": {
        "id": "0aa69a55-b3df-4127-a32a-aaf4396d1ae6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61a24bd2-8392-4940-e5fa-014ec0c0db83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URI: http://www.wikidata.org/entity/Q1126430, Label: Conny Dachs\n"
          ]
        }
      ],
      "source": [
        "# Function to extract the local part of a URI (e.g., after the last / or #)\n",
        "def extract_label_from_uri(uri, namespaces):\n",
        "    # Loop through all namespaces and remove the matching part\n",
        "    for namespace in namespaces:\n",
        "        if str(uri).startswith(str(namespace)):\n",
        "            return str(uri).replace(str(namespace), \"\")\n",
        "    # If no match, return the original URI\n",
        "    return str(uri).split('/')[-1]\n",
        "\n",
        "# Function to build a dictionary of nodes and their labels\n",
        "def build_node_label_dict(graph, namespaces):\n",
        "    nodes = {}\n",
        "\n",
        "    for node in graph.all_nodes():\n",
        "        if isinstance(node, rdflib.term.URIRef):  # Only process URIs\n",
        "            # Check if the node has a label\n",
        "            label = graph.value(node, RDFS.label)\n",
        "\n",
        "            if label:\n",
        "                # If label exists, use it\n",
        "                nodes[node.toPython()] = str(label)\n",
        "            else:\n",
        "                # If no label, extract the local part of the URI\n",
        "                local_label = extract_label_from_uri(node, namespaces)\n",
        "                nodes[node.toPython()] = local_label\n",
        "\n",
        "    return nodes\n",
        "\n",
        "namespaces = [WD, WDT, DDIS, RDFS, SCHEMA]\n",
        "\n",
        "nodes = build_node_label_dict(graph, namespaces)\n",
        "\n",
        "# Check the result\n",
        "for uri, label in nodes.items():\n",
        "    print(f\"URI: {uri}, Label: {label}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eac4d4e9-79cf-480d-892c-193616839313",
      "metadata": {
        "id": "eac4d4e9-79cf-480d-892c-193616839313"
      },
      "source": [
        "Make an inverse dictionary to find URIs of the entities given the labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "746d40d3-ea9e-4f47-97f3-08d1fc0ffa96",
      "metadata": {
        "id": "746d40d3-ea9e-4f47-97f3-08d1fc0ffa96"
      },
      "outputs": [],
      "source": [
        "ent2uri = {ent: uri for uri, ent in nodes.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8e8a1bb-a5dd-4258-b1d5-cb9573f1fd87",
      "metadata": {
        "id": "b8e8a1bb-a5dd-4258-b1d5-cb9573f1fd87"
      },
      "source": [
        "We also make another dictionary specifically for predicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f0132845-d9aa-4f67-910a-058e76b108d1",
      "metadata": {
        "id": "f0132845-d9aa-4f67-910a-058e76b108d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "063592e3-f19d-494d-e476-2556bff48f87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URI: http://www.wikidata.org/prop/direct/P58, Label: screenwriter\n"
          ]
        }
      ],
      "source": [
        "# Function to build a dictionary of predicates and their labels\n",
        "def build_pred_label_dict(graph, namespaces):\n",
        "    predicates = {}\n",
        "\n",
        "    for node in graph.predicates():\n",
        "        if isinstance(node, rdflib.term.URIRef):  # Only process URIs\n",
        "            # Check if the node has a label\n",
        "            label = graph.value(node, RDFS.label)\n",
        "\n",
        "            if label:\n",
        "                # If label exists, use it\n",
        "                predicates[node.toPython()] = str(label)\n",
        "\n",
        "            # This condition is never evaluated cause all the predicates have labels\n",
        "            else:\n",
        "                # If no label, extract the local part of the URI\n",
        "                local_label = extract_label_from_uri(node, namespaces)\n",
        "                predicates[node.toPython()] = local_label\n",
        "\n",
        "    return predicates\n",
        "\n",
        "# TODO: change the name of predicates into 'pred2lbl'\n",
        "predicates = build_pred_label_dict(graph, namespaces)\n",
        "\n",
        "# Check the result\n",
        "for uri, label in predicates.items():\n",
        "    print(f\"URI: {uri}, Label: {label}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16659397-8837-438d-910a-1190c6efbd8e",
      "metadata": {
        "id": "16659397-8837-438d-910a-1190c6efbd8e"
      },
      "source": [
        "Make an inverse dictionary to find URIs of the predicates given the labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "fad9cfe9-044f-4ff1-8213-e9fd11794d59",
      "metadata": {
        "id": "fad9cfe9-044f-4ff1-8213-e9fd11794d59"
      },
      "outputs": [],
      "source": [
        "pred2uri = {pred: uri for uri, pred in predicates.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11364b72-559d-4a6e-8c43-7df59a076234",
      "metadata": {
        "id": "11364b72-559d-4a6e-8c43-7df59a076234"
      },
      "source": [
        "## Embeddings\n",
        "\n",
        "We will now implement an approach that relies on embeddings rather than querying the graph directly. For this we will need to extract entities from the graph in a more dynamic way and will resort to NER"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2347fc4-9525-4bcc-9656-ea8f8024c0b4",
      "metadata": {
        "id": "f2347fc4-9525-4bcc-9656-ea8f8024c0b4"
      },
      "source": [
        "### NER\n",
        "\n",
        "We choose model 'Babelscape' because it was already trained on a large wikidata dataset and it is by far the best at recognizing movie titles as 'MISC'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "0f8c7a98-afc6-4053-a4bc-433f52f974ae",
      "metadata": {
        "id": "0f8c7a98-afc6-4053-a4bc-433f52f974ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3c6e479-47c6-451f-ff57-d87371c44a5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import pipeline\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
        "\n",
        "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, grouped_entities=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19a66f7d-193e-4d80-8070-d86a42143c1e",
      "metadata": {
        "id": "19a66f7d-193e-4d80-8070-d86a42143c1e"
      },
      "source": [
        "### Synonyms handling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f0c89f5-204e-44b5-a395-3cf556e627d5",
      "metadata": {
        "id": "3f0c89f5-204e-44b5-a395-3cf556e627d5"
      },
      "source": [
        "To account for the presence of synonyms in the question we decided to implement a model that computes the similarity between a phrase and the list of predicates from the knowledge graph and returns the most similar matches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "87629c72-b4f2-432a-a9dd-e36e0bd3d31e",
      "metadata": {
        "id": "87629c72-b4f2-432a-a9dd-e36e0bd3d31e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67db1890-6f5f-4479-86f2-7163542a14a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# this command downloads the Spacy model\n",
        "spacy.cli.download(\"en_core_web_md\")\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_md\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "07ce4318-a85e-42ec-b675-696284d04973",
      "metadata": {
        "id": "07ce4318-a85e-42ec-b675-696284d04973"
      },
      "outputs": [],
      "source": [
        "def find_match(phrase, predicate_dict, n=5, confidence=0.6):\n",
        "    \"\"\"\n",
        "    Given a phrase, a dictionary of predicate values, an integer n, and a confidence threshold,\n",
        "    return the top n most similar words to the phrase from the dictionary values\n",
        "    that have a similarity score above the confidence threshold.\n",
        "    \"\"\"\n",
        "    phrase_token = nlp(phrase)\n",
        "    similarities = []\n",
        "\n",
        "    # Calculate similarity between phrase and each predicate value\n",
        "    for predicate in predicate_dict.values():\n",
        "        predicate_token = nlp(predicate)\n",
        "        similarity = phrase_token.similarity(predicate_token)\n",
        "\n",
        "        # Only consider matches above the confidence threshold\n",
        "        if similarity > confidence:\n",
        "            similarities.append((predicate, similarity))\n",
        "\n",
        "    # Sort by similarity in descending order and get the top n matches\n",
        "    top_n_matches = sorted(similarities, key=lambda x: x[1], reverse=True)[:n]\n",
        "\n",
        "    # Return only the most similar words\n",
        "    return [match[0] for match in top_n_matches]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faec479c-e71c-41c3-b6ca-e430b7c0a482",
      "metadata": {
        "id": "faec479c-e71c-41c3-b6ca-e430b7c0a482"
      },
      "source": [
        "Some weakness of this methods: \"Children\" is not correctly associated to predicate \"Child\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "4633d4e7-45cf-4065-ad8d-584dec6031e2",
      "metadata": {
        "id": "4633d4e7-45cf-4065-ad8d-584dec6031e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bbb1566-ffc4-447e-e039-c49b8f9f4d54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['publication date']\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "phrase = \"release date\"\n",
        "n = 3\n",
        "print(find_match(phrase, predicates, n))  # Should return the top 3 most similar predicates"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7f687da-cc84-44de-a616-1e85fecfa47b",
      "metadata": {
        "id": "e7f687da-cc84-44de-a616-1e85fecfa47b"
      },
      "source": [
        "### EditDistance Matching\n",
        "\n",
        "For entities the problem of syninyms is not that relevent because generally we can assume that people's names and movie's titles have no synonims. However we still need to make sure that the entities recognized by the NER algorithm correspond to real entities in the knowledge graph, otherwise we cannot map them to an embedding. To achieve this we can use the match_entity function based on editdistance. This function is also useful for predicates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "id": "62b2ed57-91f4-4f26-a025-9747997efe1b",
      "metadata": {
        "id": "62b2ed57-91f4-4f26-a025-9747997efe1b"
      },
      "outputs": [],
      "source": [
        "import editdistance\n",
        "\n",
        "def match_entity_editdistance(entity, dictionary=nodes, threshold=5):\n",
        "    \"\"\"\n",
        "    Matches the given entity to the closest node in the dictionary based on edit distance.\n",
        "    Returns None if the closest match exceeds the specified distance threshold.\n",
        "\n",
        "    Args:\n",
        "    - entity (str): The entity to match.\n",
        "    - dictionary (dict): The graph dictionary with nodes to match against.\n",
        "    - threshold (int): The maximum allowable edit distance for a match.\n",
        "\n",
        "    Returns:\n",
        "    - (str, str) or (None, None): Returns (node_key, node_value) if a match is found within the threshold,\n",
        "      otherwise returns (None, None).\n",
        "    \"\"\"\n",
        "    tmp = float('inf')  # Start with the highest possible distance\n",
        "    match_node = None\n",
        "    match_value = None\n",
        "\n",
        "    for key, value in dictionary.items():\n",
        "        # Calculate edit distance between the entity and current node value\n",
        "        distance = editdistance.eval(value, entity)\n",
        "        if distance < tmp:\n",
        "            tmp = distance\n",
        "            match_node = key\n",
        "            match_value = value\n",
        "\n",
        "    # Return None if the closest match exceeds the threshold\n",
        "    if tmp > threshold:\n",
        "        return None\n",
        "\n",
        "    return match_node, match_value, tmp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "id": "665cf3c2-1bf4-4ba7-ae24-07d3a153079b",
      "metadata": {
        "id": "665cf3c2-1bf4-4ba7-ae24-07d3a153079b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "239244e2-aeef-455e-d32d-ebe3285fee58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('http://www.wikidata.org/entity/Q25188', 'Inception', 1)\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "phrase = \"Incption\"\n",
        "print(match_entity_editdistance(phrase, threshold=5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "defd30df-100e-4437-893d-7d4875e308c1",
      "metadata": {
        "id": "defd30df-100e-4437-893d-7d4875e308c1"
      },
      "source": [
        "## Extracting Predicates\n",
        "\n",
        "We have implemented the following pipeline to extract predicates from the question:\n",
        "\n",
        "- Extract meaningful words from the question with spacy\n",
        "    - For exaple: from question 'who directed...' only 'directed' is extracted\n",
        "- Generate ngrams from meaningful words\n",
        "    - If the predicate is made of 2 words like \"publication date\" the meaningful word would be ['publication', 'date'] which would not be mapped to 'publication date' but to other words. This is why we generate a list of ngrams like [\"publication 'date\", \"publication\", \"date\"].\n",
        "- Starting with the longest ngram, try to find the predicate from the predicate list that is closest to the ngram. If a match is found, we return it. This means that we prioritize matching longest ngrams\n",
        "    - Before we compare the ngram to the list of predicates we lemmatize it and turn it into a noun using the verb_to_noun dictionary we wrote. This is because many predicates in the list are in the form \"director\", \"writer\" instead of \"direct\" and \"write\"\n",
        "    - We also check if the ngram corresponds exactly to a predicate in the graph via the EditDistance matching function. In tha case the matching predicate is returned immediately\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "38618fcc-6490-42fd-95cb-a0518d4694e9",
      "metadata": {
        "id": "38618fcc-6490-42fd-95cb-a0518d4694e9"
      },
      "outputs": [],
      "source": [
        "verb_to_noun = {\n",
        "    \"affiliate\": \"affiliation\",\n",
        "    \"animate\": \"animator\",\n",
        "    \"base\": \"based on\",\n",
        "    \"cast\": \"cast member\",\n",
        "    \"characterize\": \"characters\",\n",
        "    \"depict\": \"depicts\",\n",
        "    \"describe\": \"node description\",\n",
        "    \"design\": \"designed by\",\n",
        "    \"distribute\": \"distributed by\",\n",
        "    \"educate\": \"educated at\",\n",
        "    \"employ\": \"employer\",\n",
        "    \"found\": \"founded by\",\n",
        "    \"influence\": \"influenced by\",\n",
        "    \"locate\": \"location\",\n",
        "    \"narrate\": \"narrator\",\n",
        "    \"originate\": \"country of origin\",\n",
        "    \"participate\": \"participant in\",\n",
        "    \"perform\": \"performer\",\n",
        "    \"produce\": \"producer\",\n",
        "    \"publish\": \"publication date\",\n",
        "    \"rate\": \"rating\",\n",
        "    \"receive\": \"award received\",\n",
        "    \"represent\": \"represented by\",\n",
        "    \"screen\": \"screenwriter\",\n",
        "    \"study\": \"student of\",\n",
        "    \"write\": \"screenwriter\",\n",
        "    \"direct\": \"director\",\n",
        "    \"photograph\": \"director of photography\",\n",
        "    \"edit\": \"film editor\",\n",
        "    \"speak\": \"languages spoken, written or signed\",\n",
        "    \"produce\": \"production company\",\n",
        "    \"confer\": \"conferred by\",\n",
        "    \"broadcast\": \"broadcast by\",\n",
        "    \"present\": \"presented in\",\n",
        "    \"voice\": \"voice actor\",\n",
        "    \"film\": \"filming location\",\n",
        "    \"release\": \"publication date\",\n",
        "    \"award\": \"award received\",\n",
        "    \"create\": \"creator\",\n",
        "    \"develop\": \"developer\",\n",
        "    \"choreograph\": \"choreographer\",\n",
        "    \"make\": \"production company\",\n",
        "    \"assemble\": \"crew member(s)\",\n",
        "    \"inspire\": \"inspired by\",\n",
        "    \"contribute\": \"contributor to the creative work or subject\",\n",
        "    \"style\": \"costume designer\",\n",
        "    \"nominate\": \"nominated for\",\n",
        "    \"portray\": \"cast member\",\n",
        "    \"describe\": \"node description\",\n",
        "    \"label\": \"node label\",\n",
        "    \"set\": \"narrative location\",\n",
        "    \"shot\": \"filming location\",\n",
        "    \"character\" : \"characters\",\n",
        "    \"birthplace\": \"place of birth\",\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "94979c4d-d76d-405d-b105-d99637ae9f0c",
      "metadata": {
        "id": "94979c4d-d76d-405d-b105-d99637ae9f0c"
      },
      "outputs": [],
      "source": [
        "# Check if some values in the dict do not correspond to actual entities in the graph\n",
        "for value in verb_to_noun.values():\n",
        "    if value not in predicates.values():\n",
        "        print(f\"{value} to be deleted\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4714f94e-c1ea-41c7-ac4b-a0d5a9bc5878",
      "metadata": {
        "id": "4714f94e-c1ea-41c7-ac4b-a0d5a9bc5878"
      },
      "source": [
        "### Check ngram match\n",
        "\n",
        "We are going to use the check_ngram_match match also to match predicates in the factual question part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "6924f30a-08c2-4ef2-9e08-db9bf784ea83",
      "metadata": {
        "id": "6924f30a-08c2-4ef2-9e08-db9bf784ea83"
      },
      "outputs": [],
      "source": [
        "def check_ngram_match(ngram, predicate_dict, threshold=2, n=5, confidence=0.6):\n",
        "    \"\"\"\n",
        "    Checks if an n-gram closely matches a predicate in the dictionary.\n",
        "    First, it attempts an exact or close match based on edit distance.\n",
        "    If no close match is found, it falls back to finding the best similarity match.\n",
        "\n",
        "    Args:\n",
        "    - ngram (str): The n-gram to check.\n",
        "    - predicate_dict (dict): Dictionary of known predicates.\n",
        "    - threshold (int): Maximum edit distance for an exact match.\n",
        "    - n (int): Number of top matches to return for similarity matching.\n",
        "    - confidence (float): Minimum similarity threshold for a match.\n",
        "\n",
        "    Returns:\n",
        "    - list: List containing the best-matching predicate or an empty list if no match is found.\n",
        "    \"\"\"\n",
        "    # Check if the ngram matches a predicate exactly or almost exactly\n",
        "    if match_entity_editdistance(ngram, dictionary=predicate_dict, threshold=threshold):\n",
        "        match_node, match_value, _ = match_entity_editdistance(ngram, dictionary=predicate_dict, threshold=threshold)\n",
        "        return [match_value]\n",
        "\n",
        "    # Apply lemmatization before similarity matching\n",
        "    ngram = \" \".join([verb_to_noun.get(token.lemma_, token.lemma_) for token in nlp(ngram)])\n",
        "    matches = find_match(ngram, predicate_dict, n=n, confidence=confidence)\n",
        "\n",
        "    return matches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "b8736687-1fb3-4e6a-950d-e64b5d84550a",
      "metadata": {
        "id": "b8736687-1fb3-4e6a-950d-e64b5d84550a"
      },
      "outputs": [],
      "source": [
        "def extract_relation_embeddings(sentence, predicate_dict, n=5, confidence=0.6, max_ngram_size=3):\n",
        "    \"\"\"\n",
        "    Extracts the relation from a sentence by finding the most similar predicates,\n",
        "    prioritizing longer n-grams first. If a match with similarity > confidence\n",
        "    is found, it returns that result immediately.\n",
        "\n",
        "    Args:\n",
        "    - sentence (str): The input sentence from which to extract the relation.\n",
        "    - predicate_dict (dict): Dictionary of known predicates with their descriptions.\n",
        "    - n (int): Number of top matches to return.\n",
        "    - confidence (float): Minimum similarity threshold for a match.\n",
        "    - max_ngram_size (int): Maximum number of words in an n-gram to consider for matching.\n",
        "\n",
        "    Returns:\n",
        "    - list: Top `n` predicate matches that have a similarity score above the confidence threshold.\n",
        "    \"\"\"\n",
        "    # Step 1: Parse the sentence to filter stop words and prioritize key phrases\n",
        "    doc = nlp(sentence)\n",
        "    meaningful_words = [token.text for token in doc if not token.is_stop and token.is_alpha]\n",
        "\n",
        "    # Step 2: Generate prioritized n-grams from meaningful words (starting with the longest n-grams)\n",
        "    ngrams = []\n",
        "    for size in range(max_ngram_size, 0, -1):  # Start with larger n-grams\n",
        "        ngrams += [\" \".join(meaningful_words[i:i+size]) for i in range(len(meaningful_words) - size + 1)]\n",
        "\n",
        "    # Step 3: Check each n-gram for similarity, starting with the longest\n",
        "    for ngram in ngrams:\n",
        "        matches = check_ngram_match(ngram, predicate_dict, threshold=2, n=n, confidence=confidence)\n",
        "        if matches:\n",
        "            return matches\n",
        "\n",
        "    # Step 4: If no matches above the confidence threshold are found, return an empty list\n",
        "    return []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "c32895f7-04da-4f66-984c-d6090b0e0174",
      "metadata": {
        "id": "c32895f7-04da-4f66-984c-d6090b0e0174",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f1add11-c0ca-4cb6-9d45-163889c575a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Relation: ['director']\n"
          ]
        }
      ],
      "source": [
        "sentence = \"Who is the director of ?\"\n",
        "relation = extract_relation_embeddings(sentence, predicates, n=3, confidence=0.5)\n",
        "print(\"Extracted Relation:\", relation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f18871b8-cd18-4dc8-ab9e-5a873c0d356e",
      "metadata": {
        "id": "f18871b8-cd18-4dc8-ab9e-5a873c0d356e"
      },
      "source": [
        "## Extract embeddings from the files\n",
        "\n",
        "We extract embeddings from the files. We will explain how to use them after the process_question function. Since there is a problem with the relation embeddings we need to extract them now and account for that in the process_question function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "d908c618-8b7a-4ac0-ae5e-24d758a51e52",
      "metadata": {
        "id": "d908c618-8b7a-4ac0-ae5e-24d758a51e52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "7feffe7d-9f27-4219-b857-784a92ff4bbc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"import numpy as np\\nimport csv\\n\\nentity_matrix = np.load('/Users/gianmarcoalbano/Desktop/Advanced topics in AI/Chatbot-Project/ddis-graph-embeddings/entity_embeds.npy')\\npredicate_matrix = np.load('/Users/gianmarcoalbano/Desktop/Advanced topics in AI/Chatbot-Project/ddis-graph-embeddings/relation_embeds.npy')\\n\\nwith open('/Users/gianmarcoalbano/Desktop/Advanced topics in AI/Chatbot-Project/ddis-graph-embeddings/entity_ids.del') as ifile:\\n    ent2id = {ent: int(idx) for idx, ent in csv.reader(ifile, delimiter='\\t')}\\n    id2ent = {v: k for k, v in ent2id.items()}\\nwith open('/Users/gianmarcoalbano/Desktop/Advanced topics in AI/Chatbot-Project/ddis-graph-embeddings/relation_ids.del') as ifile:\\n    pred2id = {rel: int(idx) for idx, rel in csv.reader(ifile, delimiter='\\t')}\\n    id2pred = {v: k for k, v in pred2id.items()}\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "'''import numpy as np\n",
        "import csv\n",
        "\n",
        "entity_matrix = np.load('/Users/gianmarcoalbano/Desktop/Advanced topics in AI/Chatbot-Project/ddis-graph-embeddings/entity_embeds.npy')\n",
        "predicate_matrix = np.load('/Users/gianmarcoalbano/Desktop/Advanced topics in AI/Chatbot-Project/ddis-graph-embeddings/relation_embeds.npy')\n",
        "\n",
        "with open('/Users/gianmarcoalbano/Desktop/Advanced topics in AI/Chatbot-Project/ddis-graph-embeddings/entity_ids.del') as ifile:\n",
        "    ent2id = {ent: int(idx) for idx, ent in csv.reader(ifile, delimiter='\\t')}\n",
        "    id2ent = {v: k for k, v in ent2id.items()}\n",
        "with open('/Users/gianmarcoalbano/Desktop/Advanced topics in AI/Chatbot-Project/ddis-graph-embeddings/relation_ids.del') as ifile:\n",
        "    pred2id = {rel: int(idx) for idx, rel in csv.reader(ifile, delimiter='\\t')}\n",
        "    id2pred = {v: k for k, v in pred2id.items()}'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Abu Colab Code\n",
        "\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "file_path_embeds = \"/content/drive/My Drive/Speakeasy_Project/ddis-graph-embeddings/entity_embeds.npy\"\n",
        "entity_matrix = np.load(file_path_embeds)\n",
        "\n",
        "file_path_predicate = \"/content/drive/My Drive/Speakeasy_Project/ddis-graph-embeddings/relation_embeds.npy\"\n",
        "predicate_matrix = np.load(file_path_predicate)\n",
        "\n",
        "# Assuming 'entity_ids.del' is in the same directory as other files\n",
        "file_path_entity_ids = \"/content/drive/My Drive/Speakeasy_Project/ddis-graph-embeddings/entity_ids.del\"  # Construct the path for entity_ids.del\n",
        "\n",
        "# Use the constructed path for opening the file\n",
        "with open(file_path_entity_ids) as ifile:  # Changed 'entity_ids.del' to file_path_entity_ids\n",
        "    ent2id = {ent: int(idx) for idx, ent in csv.reader(ifile, delimiter='\\t')}\n",
        "    id2ent = {v: k for k, v in ent2id.items()}\n",
        "\n",
        "# Assuming 'relation_ids.del' is in the same directory as other files\n",
        "file_path_relation_ids = \"/content/drive/My Drive/Speakeasy_Project/ddis-graph-embeddings/relation_ids.del\"  # Construct the path for relation_ids.del\n",
        "\n",
        "# Use the constructed path for opening the file\n",
        "with open(file_path_relation_ids) as ifile:  # Changed 'relation_ids.del' to file_path_relation_ids\n",
        "    pred2id = {rel: int(idx) for idx, rel in csv.reader(ifile, delimiter='\\t')}\n",
        "    id2pred = {v: k for k, v in pred2id.items()}"
      ],
      "metadata": {
        "id": "L2fM3jQyyJKi"
      },
      "id": "L2fM3jQyyJKi",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "14ceafa6-7190-4372-bc0a-b30315b2be91",
      "metadata": {
        "id": "14ceafa6-7190-4372-bc0a-b30315b2be91"
      },
      "source": [
        "### Predicates without embeddings\n",
        "\n",
        "There seems to be a problem with the embeddings. Some of them are missing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "462cbdad-4149-4094-b0d6-1ae87cdbfeaa",
      "metadata": {
        "id": "462cbdad-4149-4094-b0d6-1ae87cdbfeaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ec54b1f-5c12-4810-977c-eb42c8419762"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predicates list: 255\n",
            "relation embeddings list: 248\n",
            "\n",
            "node label has no embedding\n",
            "image has no embedding\n",
            "tag has no embedding\n",
            "node description has no embedding\n",
            "IMDb ID has no embedding\n",
            "publication date has no embedding\n",
            "rating has no embedding\n",
            "box office has no embedding\n"
          ]
        }
      ],
      "source": [
        "print(f\"predicates list: {len(predicates)}\")\n",
        "print(f\"relation embeddings list: {len(predicate_matrix)}\\n\")\n",
        "#pred2id['http://www.wikidata.org/prop/direct/P577']\n",
        "pred_without_embeddings = []\n",
        "# Which predicates are missing an embedding?\n",
        "for predicate in predicates.values():\n",
        "    try:\n",
        "        id = pred2id[pred2uri[predicate]]\n",
        "    except KeyError:\n",
        "        print(f\"{predicate} has no embedding\")\n",
        "        pred_without_embeddings.append(predicate)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a03a67f2-8fbe-422e-ae37-4124c51f4dbd",
      "metadata": {
        "id": "a03a67f2-8fbe-422e-ae37-4124c51f4dbd"
      },
      "source": [
        "# Error handling\n",
        "\n",
        "If our model is unable to find the answer we provide a human like response using the paraphrasing model \"pegasus\". Here is a dimostration of how we plan to use it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "de699fa4-7b78-4cff-afce-566d38491448",
      "metadata": {
        "id": "de699fa4-7b78-4cff-afce-566d38491448",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7409c74f-f0dc-4cad-9eb7-72d3497cd4b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at tuner007/pegasus_paraphrase and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "model_name = 'tuner007/pegasus_paraphrase'\n",
        "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
        "\n",
        "def get_response(input_text,num_return_sequences,num_beams):\n",
        "  batch = tokenizer([input_text],truncation=True,padding='longest',max_length=60, return_tensors=\"pt\").to(torch_device)\n",
        "  translated = model.generate(**batch,max_length=60,num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5)\n",
        "  tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
        "  return tgt_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "cf054d97-7c07-412a-97d4-7487c28bceff",
      "metadata": {
        "id": "cf054d97-7c07-412a-97d4-7487c28bceff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "796237b7-d451-4b43-95a6-df0105a50911"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"I don't know who the main character is in The Masked Gang: Cyprus.\",\n",
              " \"I don't know who the main character of The Masked Gang is.\",\n",
              " \"I don't know who the main character is of The Masked Gang: Cyprus.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "question = \"Who is the main character of The Masked Gang: Cyprus? \"\n",
        "num_beams = 10\n",
        "num_return_sequences = 3\n",
        "context = f\"{question} i don't know\"\n",
        "get_response(context,num_return_sequences,num_beams)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c0854b4-dae3-47d9-82d5-8bbe93e8fd20",
      "metadata": {
        "id": "0c0854b4-dae3-47d9-82d5-8bbe93e8fd20"
      },
      "source": [
        "# Handle multiple questions at once\n",
        "\n",
        "We also implemented a way to split the question into 2 subquestions using spacy, so that we can answer them separately"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "bd92bbd7-256b-4453-bfc8-d0ef88ce83b3",
      "metadata": {
        "id": "bd92bbd7-256b-4453-bfc8-d0ef88ce83b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c10137ff-605b-42dd-e8d0-b5ddaed6f0f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Who is the director of Star Wars', 'who is the screenwriter of inception']\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "def split_questions(text):\n",
        "    doc = nlp(text)\n",
        "    questions = []\n",
        "    current_question = []\n",
        "\n",
        "    for token in doc:\n",
        "        # Add token to the current question\n",
        "        current_question.append(token.text)\n",
        "\n",
        "        # If token is a conjunction like 'and', treat it as a potential separator\n",
        "        if token.dep_ == \"cc\" and token.text.lower() == \"and\":\n",
        "            # Join tokens accumulated so far and start a new question\n",
        "            questions.append(\" \".join(current_question[:-1]))\n",
        "            current_question = []\n",
        "\n",
        "    # Add the final question after loop ends\n",
        "    if current_question:\n",
        "        questions.append(\" \".join(current_question))\n",
        "\n",
        "    return [q.strip() for q in questions if q.strip()]\n",
        "    #return text\n",
        "\n",
        "# Example compound question\n",
        "question = \"Who is the director of Star Wars and who is the screenwriter of inception\"\n",
        "split_questions_list = split_questions(question)\n",
        "\n",
        "# Result\n",
        "print(split_questions_list)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2751b60-36e8-423a-a26a-1f1d39f9cef2",
      "metadata": {
        "id": "e2751b60-36e8-423a-a26a-1f1d39f9cef2"
      },
      "source": [
        "# NER Pipeline\n",
        "\n",
        "We can now combine all these functions to successfully extract both predicates and entities from a question. We will use the model \"...\" (we can still decide on a different model) to recognize entities and proceed in the following way:\n",
        "- Preprocess the question so that special characters that hold no important meaning like ! or : are removed\n",
        "- Extract a list of dictionaries with all the entities from the question using NER\n",
        "    - The dictionaries will look like: {'entity_group' : 'PER', 'word': Andrew Garfield'}\n",
        "- Map the extracted entities to actual nodes in the graph via the editdistance function\n",
        "    - If the distance from the entity in the question and the closest entity in the graph is > 5 then no entity is matched\n",
        "    - If the distance from the entity in the question and the closest entity in the graph is < 5 but > 1 then we prompt the chatbot to ask the user to verify if they matched the right enitity\n",
        "- Remove the entities from the question\n",
        "- Pass the question without entities to the predicate_extraction function\n",
        "- Add the extracted predicates to the list of dictionaries as {'entity_group' : 'predicate', 'word': 'screenwriter'}\n",
        "\n",
        "\n",
        "Some notes on how to handle the 'Entity matching too distant' case. In the final notebook with the speakeasy infrastructure you should make a variable with the matched entities that were too distant. so that they are stored for generating the next message in case they answer 'yes' to the question 'did you mean -matched_entity-?'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "49825084-47f4-4957-9c09-854579ad3e10",
      "metadata": {
        "id": "49825084-47f4-4957-9c09-854579ad3e10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31323850-295d-483c-e3b4-42dcaf37abf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import pipeline\n",
        "import re\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
        "\n",
        "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, grouped_entities=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "8bea2def-2c13-48e7-863a-c20ee803e50a",
      "metadata": {
        "id": "8bea2def-2c13-48e7-863a-c20ee803e50a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "outputId": "8a0263d3-aa93-4a64-83e5-0306f7c16684"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'import re\\n\\ndef preprocess_question_embeddings(question):\\n    # Remove symbols like :, !, -, etc., by replacing them with an empty string\\n    cleaned_question = re.sub(r\\'[:!\\\\-]\\', \\'\\', question)\\n    # Remove any extra spaces that might result from removing symbols\\n    cleaned_question = re.sub(r\\'\\\\s+\\', \\' \\', cleaned_question).strip()\\n    return cleaned_question\\n\\n# Define a function to extract entities and relation from a given question\\ndef extract_entities_NER(question, predicate_dict=predicates, n=5, confidence=0.5, max_ngram_size=3):\\n    all_extracted_entities = []  # Create a list to store all extracted entities\\n    extracted_entities = []\\n\\n    exit_status = \"\"\\n    question = preprocess_question_embeddings(question)\\n    print(f\"Question after preprocessing: {question}\\n\")\\n\\n    # Step 1: Use the NER pipeline to get entities in the question\\n    entities = ner_pipeline(question)\\n\\n    recommendation_keywords = [\"Recommend\", \"recommend\", \"Recommendation\", \"recommendation\", \"Suggest\", \"suggest\", \"Suggestion\", \"suggestion\", \"like\"]\\n\\n\\n    # If there are no entities in the question return (maybe prompt the user to double check if the capitalized the right letters)\\n    if entities:\\n\\n        # Step 2: Turn dictionaries in entities into simplified dictionaries and concatenate words to join_entity[\\'word\\']\\n        for entity in entities:\\n            simplified_entity = {\\n                \\'entity_group\\': entity[\\'entity_group\\'],\\n                \\'word\\': entity[\\'word\\']\\n            }\\n            extracted_entities.append(simplified_entity)\\n\\n\\n\\n        # Step 3: Remove extracted entities from the question to isolate the predicate phrase\\n        question_no_entities = question\\n        for entity in extracted_entities:\\n            print(f\"Extracted entity: {entity[\\'word\\']}\\n\")\\n            # Convert both the question and entity to lowercase for consistent replacement\\n            question_no_entities = re.sub(r\\'\\x08\\' + re.escape(entity[\\'word\\'].lower()) + r\\'\\x08\\', \\'\\', question_no_entities.lower(), flags=re.IGNORECASE)\\n\\n        # Replace multiple spaces with a single space and trim leading/trailing whitespace\\n        question_no_entities = re.sub(r\\'\\\\s+\\', \\' \\', question_no_entities).strip()\\n\\n        print(f\"Question after removing entities: {question_no_entities}\\n\")\\n\\n        # Step 3.5: Match each entity to the closest node in the graph. Remove them if there is no match\\n        for entity in extracted_entities:\\n\\n            if match_entity_editdistance(entity[\\'word\\'], threshold=5):\\n                match_node, match_value, distance = match_entity_editdistance(entity[\\'word\\'])\\n\\n                # If the closest entity we can find in the graph is still distant, return the best matched value\\n                # with exit status Entity matching too distant. Then ask the user if the match_value actually\\n                # corresponds to what they wanted\\n                if distance > 5:\\n                    exit_status = \\'Entity matching too distant\\'\\n                    return match_value, exit_status\\n                else:\\n                    # Update \\'word\\' in entity to be the best-matching node\\'s label\\n                    entity[\\'word\\'] = match_value\\n            else:\\n                # Remove the entity from extracted_entities if no match was found\\n                extracted_entities.remove(entity)\\n\\n    else:\\n        exit_status = \\'No entities found by NER\\'\\n\\n    liked_movies = []\\n    if any(keyword in question for keyword in recommendation_keywords):\\n        #extracted_entities.append({\\'entity_group\\': \\'recommend_action\\', \\'word\\': \\'recommend\\'})\\n        liked_movies = [entity[\\'word\\'] for entity in extracted_entities if entity[\\'entity_group\\'] == \\'MISC\\']\\n        print(f\"Liked movies: {liked_movies}\\n\")\\n        return liked_movies, exit_status\\n\\n\\n\\n    # Step 4: Extract the relation from the modified question using the extract_relation function\\n    relations = extract_relation_embeddings(question_no_entities if entities else question, predicates, n=n, confidence=confidence, max_ngram_size=max_ngram_size)\\n\\n    # Step 4.5: Check if the extracted relations have an embedding\\n    for relation in relations:\\n        if relation in pred_without_embeddings:\\n            exit_status = \\'predicate missing embedding\\'\\n            return relation, exit_status\\n\\n    # Step 5: Add the relation to the extracted_entities list if a match is found\\n    #if relations:\\n        #print(f\"Extracted predicates: {relations}\\n\")\\n        #extracted_entities.append({\\'entity_group\\': \\'predicate\\', \\'word\\': []})\\n        #for relation in relations:\\n            #extracted_entities[-1][\\'word\\'].append(relation)\\n\\n    if relations:\\n        extracted_entities.append({\\'entity_group\\': \\'predicate\\', \\'word\\': []})\\n        for relation in relations:\\n            if relation in pred2uri and pred2uri[relation] in pred2id:  # Check if relation has embedding\\n                extracted_entities[-1][\\'word\\'].append(relation)\\n\\n    #return extracted_entities, exit_status\\n\\n\\n    for entity_dict in extracted_entities:\\n        all_extracted_entities.extend(entity_dict[\\'word\\'] if isinstance(entity_dict[\\'word\\'], list) else [entity_dict[\\'word\\']])\\n\\n    if exit_status in [\\'Entity matching too distant\\', \\'predicate missing embedding\\', \\'No entities found by NER\\']:\\n        return extracted_entities, exit_status, []  # Return an empty list for all_extracted_entities\\n    else:\\n        return extracted_entities, exit_status, all_extracted_entities\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "'''import re\n",
        "\n",
        "def preprocess_question_embeddings(question):\n",
        "    # Remove symbols like :, !, -, etc., by replacing them with an empty string\n",
        "    cleaned_question = re.sub(r'[:!\\\\-]', '', question)\n",
        "    # Remove any extra spaces that might result from removing symbols\n",
        "    cleaned_question = re.sub(r'\\s+', ' ', cleaned_question).strip()\n",
        "    return cleaned_question\n",
        "\n",
        "# Define a function to extract entities and relation from a given question\n",
        "def extract_entities_NER(question, predicate_dict=predicates, n=5, confidence=0.5, max_ngram_size=3):\n",
        "    all_extracted_entities = []  # Create a list to store all extracted entities\n",
        "    extracted_entities = []\n",
        "\n",
        "    exit_status = \"\"\n",
        "    question = preprocess_question_embeddings(question)\n",
        "    print(f\"Question after preprocessing: {question}\\n\")\n",
        "\n",
        "    # Step 1: Use the NER pipeline to get entities in the question\n",
        "    entities = ner_pipeline(question)\n",
        "\n",
        "    recommendation_keywords = [\"Recommend\", \"recommend\", \"Recommendation\", \"recommendation\", \"Suggest\", \"suggest\", \"Suggestion\", \"suggestion\", \"like\"]\n",
        "\n",
        "\n",
        "    # If there are no entities in the question return (maybe prompt the user to double check if the capitalized the right letters)\n",
        "    if entities:\n",
        "\n",
        "        # Step 2: Turn dictionaries in entities into simplified dictionaries and concatenate words to join_entity['word']\n",
        "        for entity in entities:\n",
        "            simplified_entity = {\n",
        "                'entity_group': entity['entity_group'],\n",
        "                'word': entity['word']\n",
        "            }\n",
        "            extracted_entities.append(simplified_entity)\n",
        "\n",
        "\n",
        "\n",
        "        # Step 3: Remove extracted entities from the question to isolate the predicate phrase\n",
        "        question_no_entities = question\n",
        "        for entity in extracted_entities:\n",
        "            print(f\"Extracted entity: {entity['word']}\\n\")\n",
        "            # Convert both the question and entity to lowercase for consistent replacement\n",
        "            question_no_entities = re.sub(r'\\b' + re.escape(entity['word'].lower()) + r'\\b', '', question_no_entities.lower(), flags=re.IGNORECASE)\n",
        "\n",
        "        # Replace multiple spaces with a single space and trim leading/trailing whitespace\n",
        "        question_no_entities = re.sub(r'\\s+', ' ', question_no_entities).strip()\n",
        "\n",
        "        print(f\"Question after removing entities: {question_no_entities}\\n\")\n",
        "\n",
        "        # Step 3.5: Match each entity to the closest node in the graph. Remove them if there is no match\n",
        "        for entity in extracted_entities:\n",
        "\n",
        "            if match_entity_editdistance(entity['word'], threshold=5):\n",
        "                match_node, match_value, distance = match_entity_editdistance(entity['word'])\n",
        "\n",
        "                # If the closest entity we can find in the graph is still distant, return the best matched value\n",
        "                # with exit status Entity matching too distant. Then ask the user if the match_value actually\n",
        "                # corresponds to what they wanted\n",
        "                if distance > 5:\n",
        "                    exit_status = 'Entity matching too distant'\n",
        "                    return match_value, exit_status\n",
        "                else:\n",
        "                    # Update 'word' in entity to be the best-matching node's label\n",
        "                    entity['word'] = match_value\n",
        "            else:\n",
        "                # Remove the entity from extracted_entities if no match was found\n",
        "                extracted_entities.remove(entity)\n",
        "\n",
        "    else:\n",
        "        exit_status = 'No entities found by NER'\n",
        "\n",
        "    liked_movies = []\n",
        "    if any(keyword in question for keyword in recommendation_keywords):\n",
        "        #extracted_entities.append({'entity_group': 'recommend_action', 'word': 'recommend'})\n",
        "        liked_movies = [entity['word'] for entity in extracted_entities if entity['entity_group'] == 'MISC']\n",
        "        print(f\"Liked movies: {liked_movies}\\n\")\n",
        "        return liked_movies, exit_status\n",
        "\n",
        "\n",
        "\n",
        "    # Step 4: Extract the relation from the modified question using the extract_relation function\n",
        "    relations = extract_relation_embeddings(question_no_entities if entities else question, predicates, n=n, confidence=confidence, max_ngram_size=max_ngram_size)\n",
        "\n",
        "    # Step 4.5: Check if the extracted relations have an embedding\n",
        "    for relation in relations:\n",
        "        if relation in pred_without_embeddings:\n",
        "            exit_status = 'predicate missing embedding'\n",
        "            return relation, exit_status\n",
        "\n",
        "    # Step 5: Add the relation to the extracted_entities list if a match is found\n",
        "    #if relations:\n",
        "        #print(f\"Extracted predicates: {relations}\\n\")\n",
        "        #extracted_entities.append({'entity_group': 'predicate', 'word': []})\n",
        "        #for relation in relations:\n",
        "            #extracted_entities[-1]['word'].append(relation)\n",
        "\n",
        "    if relations:\n",
        "        extracted_entities.append({'entity_group': 'predicate', 'word': []})\n",
        "        for relation in relations:\n",
        "            if relation in pred2uri and pred2uri[relation] in pred2id:  # Check if relation has embedding\n",
        "                extracted_entities[-1]['word'].append(relation)\n",
        "\n",
        "    #return extracted_entities, exit_status\n",
        "\n",
        "\n",
        "    for entity_dict in extracted_entities:\n",
        "        all_extracted_entities.extend(entity_dict['word'] if isinstance(entity_dict['word'], list) else [entity_dict['word']])\n",
        "\n",
        "    if exit_status in ['Entity matching too distant', 'predicate missing embedding', 'No entities found by NER']:\n",
        "        return extracted_entities, exit_status, []  # Return an empty list for all_extracted_entities\n",
        "    else:\n",
        "        return extracted_entities, exit_status, all_extracted_entities\n",
        "'''\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''def extract_entities_NER(question, predicate_dict=predicates, n=5, confidence=0.5, max_ngram_size=3):\n",
        "    all_extracted_entities = []\n",
        "    extracted_entities = []\n",
        "    exit_status = \"\"\n",
        "\n",
        "    # Preprocess the question\n",
        "    question = preprocess_question_embeddings(question)\n",
        "    print(f\"DEBUG: Question after preprocessing: {question}\\n\")\n",
        "\n",
        "    # Extract entities using NER pipeline\n",
        "    entities = ner_pipeline(question)\n",
        "    print(f\"DEBUG: Entities extracted by NER: {entities}\\n\")\n",
        "\n",
        "    recommendation_keywords = [\"Recommend\", \"recommend\", \"Recommendation\", \"recommendation\",\n",
        "                               \"Suggest\", \"suggest\", \"Suggestion\", \"suggestion\", \"like\"]\n",
        "\n",
        "    if entities:\n",
        "        # Process extracted entities\n",
        "        for entity in entities:\n",
        "            if entity['entity_group'] == 'MISC' and entity['score'] > 0.4:\n",
        "                simplified_entity = {\n",
        "                    'entity_group': entity['entity_group'],\n",
        "                    'word': entity['word']\n",
        "                }\n",
        "                extracted_entities.append(simplified_entity)\n",
        "                all_extracted_entities.append(entity['word'])\n",
        "        print(f\"DEBUG: Extracted entities: {extracted_entities}\\n\")\n",
        "        print(f\"DEBUG: All extracted entities after adding entities: {all_extracted_entities}\\n\")\n",
        "\n",
        "        # Remove entities from question for predicate extraction\n",
        "        question_no_entities = question\n",
        "        for entity in extracted_entities[:]:\n",
        "            print(f\"DEBUG: Processing entity: {entity['word']}\\n\")\n",
        "            question_no_entities = re.sub(r'\\b' + re.escape(entity['word'].lower()) + r'\\b', '', question_no_entities.lower(), flags=re.IGNORECASE)\n",
        "\n",
        "            # Match entity to graph nodes\n",
        "            match = match_entity_editdistance(entity['word'], threshold=5)\n",
        "            if match:\n",
        "                match_node, match_value, distance = match\n",
        "                print(f\"DEBUG: Entity '{entity['word']}' matched with '{match_value}' at distance {distance}\\n\")\n",
        "                if distance > 5:\n",
        "                    exit_status = 'Entity matching too distant'\n",
        "                    all_extracted_entities = [match_value if x == entity['word'] else x for x in all_extracted_entities]\n",
        "                    print(f\"DEBUG: Exit status due to distant entity: {exit_status}\\n\")\n",
        "                    return extracted_entities, exit_status, all_extracted_entities\n",
        "                else:\n",
        "                    entity['word'] = match_value\n",
        "                    all_extracted_entities = [match_value if x == entity['word'] else x for x in all_extracted_entities]\n",
        "            else:\n",
        "                print(f\"DEBUG: No match found for entity '{entity['word']}'. Removing it.\\n\")\n",
        "                extracted_entities.remove(entity)\n",
        "\n",
        "        question_no_entities = re.sub(r'\\s+', ' ', question_no_entities).strip()\n",
        "        print(f\"DEBUG: Question after removing entities: {question_no_entities}\\n\")\n",
        "    else:\n",
        "        exit_status = 'No entities found by NER'\n",
        "        print(f\"DEBUG: Exit status due to no entities: {exit_status}\\n\")\n",
        "        return [], exit_status, []\n",
        "\n",
        "    # Handle recommendation-related keywords\n",
        "    liked_movies = []\n",
        "    if any(keyword in question for keyword in recommendation_keywords):\n",
        "        liked_movies = [entity['word'] for entity in extracted_entities if entity['entity_group'] == 'MISC']\n",
        "        print(f\"DEBUG: Liked movies: {liked_movies}\\n\")\n",
        "        return liked_movies, exit_status, liked_movies\n",
        "\n",
        "    # Extract relations (predicates)\n",
        "    relations = extract_relation_embeddings(question_no_entities if entities else question,\n",
        "                                            predicates, n=n, confidence=confidence,\n",
        "                                            max_ngram_size=max_ngram_size)\n",
        "    print(f\"DEBUG: Relations extracted: {relations}\\n\")\n",
        "\n",
        "    if relations:\n",
        "        # Add extracted predicates to extracted_entities\n",
        "        for relation in relations:\n",
        "            # Add relation as a predicate entity to extracted_entities\n",
        "            extracted_entities.append({'entity_group': 'predicate', 'word': relation})\n",
        "            print(f\"DEBUG: Added predicate '{relation}' to extracted_entities.\\n\")\n",
        "\n",
        "    # Append predicates to all_extracted_entities\n",
        "    for entity_dict in extracted_entities:\n",
        "        if isinstance(entity_dict['word'], list):\n",
        "            all_extracted_entities.extend(entity_dict['word'])\n",
        "        else:\n",
        "            all_extracted_entities.append(entity_dict['word'])\n",
        "    print(f\"DEBUG: Final all_extracted_entities: {all_extracted_entities}\\n\")\n",
        "\n",
        "    return extracted_entities, exit_status, all_extracted_entities\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "Ce9rZIciWDZY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "outputId": "4739fcd1-dec8-47d6-c6a2-ddb9152079a5"
      },
      "id": "Ce9rZIciWDZY",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'def extract_entities_NER(question, predicate_dict=predicates, n=5, confidence=0.5, max_ngram_size=3):\\n    all_extracted_entities = []\\n    extracted_entities = []\\n    exit_status = \"\"\\n\\n    # Preprocess the question\\n    question = preprocess_question_embeddings(question)\\n    print(f\"DEBUG: Question after preprocessing: {question}\\n\")\\n\\n    # Extract entities using NER pipeline\\n    entities = ner_pipeline(question)\\n    print(f\"DEBUG: Entities extracted by NER: {entities}\\n\")\\n\\n    recommendation_keywords = [\"Recommend\", \"recommend\", \"Recommendation\", \"recommendation\",\\n                               \"Suggest\", \"suggest\", \"Suggestion\", \"suggestion\", \"like\"]\\n\\n    if entities:\\n        # Process extracted entities\\n        for entity in entities:\\n            if entity[\\'entity_group\\'] == \\'MISC\\' and entity[\\'score\\'] > 0.4:\\n                simplified_entity = {\\n                    \\'entity_group\\': entity[\\'entity_group\\'],\\n                    \\'word\\': entity[\\'word\\']\\n                }\\n                extracted_entities.append(simplified_entity)\\n                all_extracted_entities.append(entity[\\'word\\'])\\n        print(f\"DEBUG: Extracted entities: {extracted_entities}\\n\")\\n        print(f\"DEBUG: All extracted entities after adding entities: {all_extracted_entities}\\n\")\\n\\n        # Remove entities from question for predicate extraction\\n        question_no_entities = question\\n        for entity in extracted_entities[:]:\\n            print(f\"DEBUG: Processing entity: {entity[\\'word\\']}\\n\")\\n            question_no_entities = re.sub(r\\'\\x08\\' + re.escape(entity[\\'word\\'].lower()) + r\\'\\x08\\', \\'\\', question_no_entities.lower(), flags=re.IGNORECASE)\\n\\n            # Match entity to graph nodes\\n            match = match_entity_editdistance(entity[\\'word\\'], threshold=5)\\n            if match:\\n                match_node, match_value, distance = match\\n                print(f\"DEBUG: Entity \\'{entity[\\'word\\']}\\' matched with \\'{match_value}\\' at distance {distance}\\n\")\\n                if distance > 5:\\n                    exit_status = \\'Entity matching too distant\\'\\n                    all_extracted_entities = [match_value if x == entity[\\'word\\'] else x for x in all_extracted_entities]\\n                    print(f\"DEBUG: Exit status due to distant entity: {exit_status}\\n\")\\n                    return extracted_entities, exit_status, all_extracted_entities\\n                else:\\n                    entity[\\'word\\'] = match_value\\n                    all_extracted_entities = [match_value if x == entity[\\'word\\'] else x for x in all_extracted_entities]\\n            else:\\n                print(f\"DEBUG: No match found for entity \\'{entity[\\'word\\']}\\'. Removing it.\\n\")\\n                extracted_entities.remove(entity)\\n\\n        question_no_entities = re.sub(r\\'\\\\s+\\', \\' \\', question_no_entities).strip()\\n        print(f\"DEBUG: Question after removing entities: {question_no_entities}\\n\")\\n    else:\\n        exit_status = \\'No entities found by NER\\'\\n        print(f\"DEBUG: Exit status due to no entities: {exit_status}\\n\")\\n        return [], exit_status, []\\n\\n    # Handle recommendation-related keywords\\n    liked_movies = []\\n    if any(keyword in question for keyword in recommendation_keywords):\\n        liked_movies = [entity[\\'word\\'] for entity in extracted_entities if entity[\\'entity_group\\'] == \\'MISC\\']\\n        print(f\"DEBUG: Liked movies: {liked_movies}\\n\")\\n        return liked_movies, exit_status, liked_movies\\n\\n    # Extract relations (predicates)\\n    relations = extract_relation_embeddings(question_no_entities if entities else question,\\n                                            predicates, n=n, confidence=confidence,\\n                                            max_ngram_size=max_ngram_size)\\n    print(f\"DEBUG: Relations extracted: {relations}\\n\")\\n\\n    if relations:\\n        # Add extracted predicates to extracted_entities\\n        for relation in relations:\\n            # Add relation as a predicate entity to extracted_entities\\n            extracted_entities.append({\\'entity_group\\': \\'predicate\\', \\'word\\': relation})\\n            print(f\"DEBUG: Added predicate \\'{relation}\\' to extracted_entities.\\n\")\\n\\n    # Append predicates to all_extracted_entities\\n    for entity_dict in extracted_entities:\\n        if isinstance(entity_dict[\\'word\\'], list):\\n            all_extracted_entities.extend(entity_dict[\\'word\\'])\\n        else:\\n            all_extracted_entities.append(entity_dict[\\'word\\'])\\n    print(f\"DEBUG: Final all_extracted_entities: {all_extracted_entities}\\n\")\\n\\n    return extracted_entities, exit_status, all_extracted_entities\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def preprocess_question_embeddings(question):\n",
        "    # Remove symbols like :, !, -, etc., by replacing them with an empty string\n",
        "    cleaned_question = re.sub(r'[:!\\\\-]', '', question)\n",
        "    # Remove any extra spaces that might result from removing symbols\n",
        "    cleaned_question = re.sub(r'\\s+', ' ', cleaned_question).strip()\n",
        "    return cleaned_question\n",
        "\n",
        "def extract_entities_NER(question, predicate_dict=predicates, n=5, confidence=0.5, max_ngram_size=3):\n",
        "    all_extracted_entities = []\n",
        "    extracted_entities = []\n",
        "    exit_status = \"\"\n",
        "\n",
        "    # Preprocess the question\n",
        "    question = preprocess_question_embeddings(question)\n",
        "    #print(f\"DEBUG: Question after preprocessing: {question}\\n\")\n",
        "\n",
        "    # Extract entities using NER pipeline\n",
        "    entities = ner_pipeline(question)\n",
        "    #print(f\"DEBUG: Entities extracted by NER: {entities}\\n\")\n",
        "\n",
        "    recommendation_keywords = [\"Recommend\", \"recommend\", \"Recommendation\", \"recommendation\",\n",
        "                               \"Suggest\", \"suggest\", \"Suggestion\", \"suggestion\", \"like\"]\n",
        "\n",
        "    if entities:\n",
        "        # Process extracted entities\n",
        "        for entity in entities:\n",
        "            if entity['entity_group'] == 'MISC' and entity['score'] > 0.4:\n",
        "                simplified_entity = {\n",
        "                    'entity_group': entity['entity_group'],\n",
        "                    'word': entity['word']\n",
        "                }\n",
        "                # Check if the entity has already been added to all_extracted_entities\n",
        "                if entity['word'] not in all_extracted_entities:\n",
        "                    extracted_entities.append(simplified_entity)\n",
        "                    all_extracted_entities.append(entity['word'])\n",
        "        #print(f\"DEBUG: Extracted entities: {extracted_entities}\\n\")\n",
        "        #print(f\"DEBUG: All extracted entities after adding entities: {all_extracted_entities}\\n\")\n",
        "\n",
        "        # Remove entities from question for predicate extraction\n",
        "        question_no_entities = question\n",
        "        for entity in extracted_entities[:]:\n",
        "            #print(f\"DEBUG: Processing entity: {entity['word']}\\n\")\n",
        "            question_no_entities = re.sub(r'\\b' + re.escape(entity['word'].lower()) + r'\\b', '', question_no_entities.lower(), flags=re.IGNORECASE)\n",
        "\n",
        "            # Match entity to graph nodes\n",
        "            match = match_entity_editdistance(entity['word'], threshold=5)\n",
        "            if match:\n",
        "                match_node, match_value, distance = match\n",
        "                #print(f\"DEBUG: Entity '{entity['word']}' matched with '{match_value}' at distance {distance}\\n\")\n",
        "                if distance > 5:\n",
        "                    exit_status = 'Entity matching too distant'\n",
        "                    all_extracted_entities = [match_value if x == entity['word'] else x for x in all_extracted_entities]\n",
        "                    #print(f\"DEBUG: Exit status due to distant entity: {exit_status}\\n\")\n",
        "                    return extracted_entities, exit_status, all_extracted_entities\n",
        "                else:\n",
        "                    entity['word'] = match_value\n",
        "                    all_extracted_entities = [match_value if x == entity['word'] else x for x in all_extracted_entities]\n",
        "            else:\n",
        "                #print(f\"DEBUG: No match found for entity '{entity['word']}'. Removing it.\\n\")\n",
        "                extracted_entities.remove(entity)\n",
        "\n",
        "        question_no_entities = re.sub(r'\\s+', ' ', question_no_entities).strip()\n",
        "        #print(f\"DEBUG: Question after removing entities: {question_no_entities}\\n\")\n",
        "    else:\n",
        "        exit_status = 'No entities found by NER'\n",
        "        #print(f\"DEBUG: Exit status due to no entities: {exit_status}\\n\")\n",
        "        return [], exit_status, []\n",
        "\n",
        "    # Handle recommendation-related keywords\n",
        "    liked_movies = []\n",
        "    if any(keyword in question for keyword in recommendation_keywords):\n",
        "        liked_movies = [entity['word'] for entity in extracted_entities if entity['entity_group'] == 'MISC']\n",
        "        #print(f\"DEBUG: Liked movies: {liked_movies}\\n\")\n",
        "        return liked_movies, exit_status, liked_movies\n",
        "\n",
        "    # Extract relations (predicates)\n",
        "    relations = extract_relation_embeddings(question_no_entities if entities else question,\n",
        "                                            predicates, n=n, confidence=confidence,\n",
        "                                            max_ngram_size=max_ngram_size)\n",
        "    #print(f\"DEBUG: Relations extracted: {relations}\\n\")\n",
        "\n",
        "    if relations:\n",
        "        # Add extracted predicates to extracted_entities\n",
        "        for relation in relations:\n",
        "            # Check if the relation is already added\n",
        "            if relation not in all_extracted_entities:\n",
        "                extracted_entities.append({'entity_group': 'predicate', 'word': relation})\n",
        "                #print(f\"DEBUG: Added predicate '{relation}' to extracted_entities.\\n\")\n",
        "\n",
        "    # Append predicates to all_extracted_entities\n",
        "    for entity_dict in extracted_entities:\n",
        "        if isinstance(entity_dict['word'], list):\n",
        "            all_extracted_entities.extend(entity_dict['word'])\n",
        "        else:\n",
        "            # Ensure no duplication in all_extracted_entities\n",
        "            if entity_dict['word'] not in all_extracted_entities:\n",
        "                all_extracted_entities.append(entity_dict['word'])\n",
        "    #print(f\"DEBUG: Final all_extracted_entities: {all_extracted_entities}\\n\")\n",
        "\n",
        "    return extracted_entities, exit_status, all_extracted_entities\n"
      ],
      "metadata": {
        "id": "GhnIaX6ZxAAJ"
      },
      "id": "GhnIaX6ZxAAJ",
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "a87816e1-8fa5-4780-90c0-cc0354e8734b",
      "metadata": {
        "id": "a87816e1-8fa5-4780-90c0-cc0354e8734b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee29743a-05ce-469e-eec5-ec4287308754"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted entities: {'entity_group': 'MISC', 'word': 'The Godfather'}\n",
            "Exit status: # Example usage:\n",
            "question = \"What is the box office of The Princess and the Frog?\"\n",
            "entities = extract_and_map_entities(question)  # Assume you have this function for extracting entities\n",
            "entity_id = entities[\"entity\"]\n",
            "relation_id = entities[\"relation\"]\n",
            "\n",
            "# Step 1: Filter workers based on the time threshold only\n",
            "valid_workers = filter_workers(data, approval_threshold=60, time_threshold=5)\n",
            "\n",
            "# Step 2: Perform the crowdsourced search using the valid workers\n",
            "answer = crowdsource_search_with_valid_workers(entity_id, relation_id, valid_workers)\n",
            "print(answer)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sentence = \"Who is the director of The Godfather\"\n",
        "extracted_entities, exit_status, all_extracted_entities = extract_entities_NER(sentence, predicates, n=2, confidence=0.6)\n",
        "print(\"Extracted entities:\", extracted_entities[0])\n",
        "print(\"Exit status:\", _)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a019e4d0-cce6-4062-bf72-f35e12c32ab3",
      "metadata": {
        "id": "a019e4d0-cce6-4062-bf72-f35e12c32ab3"
      },
      "source": [
        "## Turn labels into Embeddings\n",
        "\n",
        "Now that we have a reliable way of extracting entities and predicates from the question we can turn them into embeddigs:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9eda7e3-398d-4689-86ca-6cf304f0bcac",
      "metadata": {
        "id": "e9eda7e3-398d-4689-86ca-6cf304f0bcac"
      },
      "source": [
        "ent2id can be used to retrieve the index of an entity in the embedding matrix given it's Uri. Retriving the embedding of an entity given it's label would look like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "86bc2f81-0296-479a-81fe-78a1d69a1a54",
      "metadata": {
        "scrolled": true,
        "id": "86bc2f81-0296-479a-81fe-78a1d69a1a54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efcccac1-bf25-4588-c2d8-f61247e57461"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The URI of The Godfather is http://www.wikidata.org/entity/Q1139696\n",
            "\n",
            "The id of The Godfather is 85038\n",
            "\n",
            "The embedding of The Godfather has lenght 256\n",
            "\n"
          ]
        }
      ],
      "source": [
        "entity_label = 'The Godfather'\n",
        "\n",
        "# Turn label into URI\n",
        "Uri = ent2uri[entity_label]\n",
        "print(f\"The URI of {entity_label} is {Uri}\\n\")\n",
        "\n",
        "# Turn URI into a row index\n",
        "id = ent2id[Uri]\n",
        "print(f\"The id of {entity_label} is {id}\\n\")\n",
        "\n",
        "# Look up the row index in the embedding matrix\n",
        "entity_embedding = entity_matrix[id]\n",
        "print(f\"The embedding of {entity_label} has lenght {len(entity_embedding)}\\n\") # I don't print it cause it's long\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "21633989-8062-46c9-a03a-10cc4bf2d595",
      "metadata": {
        "id": "21633989-8062-46c9-a03a-10cc4bf2d595",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "044c70be-53db-4349-f9f8-59be3a062602"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The URI of director is http://www.wikidata.org/prop/direct/P57\n",
            "\n",
            "The id of director is 12\n",
            "\n",
            "The embedding of director has lenght 256\n",
            "\n"
          ]
        }
      ],
      "source": [
        "entity_label = 'director'\n",
        "\n",
        "# Turn label into URI\n",
        "Uri = pred2uri[entity_label]\n",
        "print(f\"The URI of {entity_label} is {Uri}\\n\")\n",
        "\n",
        "# Turn URI into a row index\n",
        "id = pred2id[Uri]\n",
        "print(f\"The id of {entity_label} is {id}\\n\")\n",
        "\n",
        "# Look up the row index in the embedding matrix\n",
        "entity_embedding = predicate_matrix[id]\n",
        "print(f\"The embedding of {entity_label} has lenght {len(entity_embedding)}\\n\") # I don't print it cause it's long"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "731a787c-5bce-479a-a271-65ebca1bc815",
      "metadata": {
        "id": "731a787c-5bce-479a-a271-65ebca1bc815"
      },
      "source": [
        "### Extract embeddings\n",
        "\n",
        "We write a function to make embedding retrival more straightforward:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "ca61fa07-eaf2-4826-ae79-d189684e5024",
      "metadata": {
        "id": "ca61fa07-eaf2-4826-ae79-d189684e5024"
      },
      "outputs": [],
      "source": [
        "def extract_embedding(label, type='entity'):\n",
        "\n",
        "    if type=='entity':\n",
        "        pipeline = [ent2uri, ent2id, entity_matrix]\n",
        "    else:\n",
        "        pipeline = [pred2uri, pred2id, predicate_matrix]\n",
        "\n",
        "\n",
        "    Uri = pipeline[0][label]\n",
        "\n",
        "    # Turn URI into a row index\n",
        "    id = pipeline[1][Uri]\n",
        "\n",
        "    # Look up the row index in the embedding matrix\n",
        "    entity_embedding = pipeline[2][id]\n",
        "\n",
        "    return entity_embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "3da6cb93-a41b-4d34-9bcd-2e5f73f41bb5",
      "metadata": {
        "id": "3da6cb93-a41b-4d34-9bcd-2e5f73f41bb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce7683ad-797b-4acc-ddda-e286f1c163d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The embedding of The Godfather has lenght 256\n",
            "\n",
            "The embedding of director has lenght 256\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "\n",
        "entity_label = 'The Godfather'\n",
        "entity_embedding = extract_embedding(entity_label)\n",
        "print(f\"The embedding of {entity_label} has lenght {len(entity_embedding)}\\n\")\n",
        "\n",
        "pred_label = 'director'\n",
        "pred_embedding = extract_embedding(pred_label, 'predicate')\n",
        "print(f\"The embedding of {pred_label} has lenght {len(pred_embedding)}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb61a295-26fa-4036-9520-e8c087986c2d",
      "metadata": {
        "id": "eb61a295-26fa-4036-9520-e8c087986c2d"
      },
      "source": [
        "### Extract labels\n",
        "\n",
        "We need also a way to turn an embedding into a label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "031643c4-a994-464f-bde5-367a2dcfd8f5",
      "metadata": {
        "id": "031643c4-a994-464f-bde5-367a2dcfd8f5"
      },
      "outputs": [],
      "source": [
        "def extract_label(embedding, type='entity'):\n",
        "\n",
        "    if type=='entity':\n",
        "        pipeline = [entity_matrix, id2ent, nodes]\n",
        "    else:\n",
        "        pipeline = [predicate_matrix, id2pred, predicates]\n",
        "\n",
        "    # Find the index in the entity embeddings matrix that corresponds to the embedding vector\n",
        "    id = np.where((pipeline[0] == embedding).all(axis=1))[0][0]\n",
        "\n",
        "    # Turn the id into a URI\n",
        "    Uri = pipeline[1][id]\n",
        "\n",
        "    # Turn the URI into a label\n",
        "    label = pipeline[2][Uri]\n",
        "\n",
        "    return label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "e944b122-afd5-4c7e-80cd-b2dcbbc4cd4c",
      "metadata": {
        "id": "e944b122-afd5-4c7e-80cd-b2dcbbc4cd4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90babceb-31f8-4a8a-cf5f-70021d6db0f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The embedding of The Godfather has lenght 256\n",
            "\n",
            "The extracted label for entity: The Godfather is The Godfather\n",
            "\n",
            "The embedding of characters has lenght 256\n",
            "\n",
            "The extracted label for predicate: characters is characters\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "\n",
        "entity_label = 'The Godfather'\n",
        "entity_embedding = extract_embedding(entity_label)\n",
        "print(f\"The embedding of {entity_label} has lenght {len(entity_embedding)}\\n\")\n",
        "\n",
        "# Turn the embedding back into a label\n",
        "label = extract_label(entity_embedding)\n",
        "print(f\"The extracted label for entity: {entity_label} is {label}\\n\")\n",
        "\n",
        "pred_label = 'characters'\n",
        "pred_embedding = extract_embedding(pred_label, 'predicate')\n",
        "print(f\"The embedding of {pred_label} has lenght {len(pred_embedding)}\\n\")\n",
        "\n",
        "# Turn the embedding back into a label\n",
        "label = extract_label(pred_embedding, 'predicate')\n",
        "print(f\"The extracted label for predicate: {pred_label} is {label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8addb9fb-1645-4f32-af3d-71967175491e",
      "metadata": {
        "id": "8addb9fb-1645-4f32-af3d-71967175491e"
      },
      "source": [
        "### Evaluate embeddings similarity\n",
        "\n",
        "Given the embedding of an entity we want to find the most similar entities in the graph to said entity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "eacf5a57-56f9-4b84-a192-6da08c349738",
      "metadata": {
        "id": "eacf5a57-56f9-4b84-a192-6da08c349738"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import pairwise_distances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "5c543acc-793c-4365-9512-229c6f9162cb",
      "metadata": {
        "id": "5c543acc-793c-4365-9512-229c6f9162cb"
      },
      "outputs": [],
      "source": [
        "def find_similarities(embedding, n):\n",
        "\n",
        "    embedding = np.atleast_2d(embedding)\n",
        "\n",
        "    answer = []\n",
        "\n",
        "    dist = pairwise_distances(embedding, entity_matrix)\n",
        "    for idx in dist.argsort().reshape(-1)[:n]:\n",
        "        answer.append(nodes[id2ent[idx]])\n",
        "\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "cf43a18f-d845-43f4-81f1-f87c1d9764d6",
      "metadata": {
        "scrolled": true,
        "id": "cf43a18f-d845-43f4-81f1-f87c1d9764d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd96d4cf-764d-4e59-c2d8-eb51e857b379"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Batman', 'Deathstroke', 'Harley Quinn', 'The Joker', 'Killer Croc']\n"
          ]
        }
      ],
      "source": [
        "# Example Usage\n",
        "\n",
        "entity_embedding = extract_embedding('Batman')\n",
        "\n",
        "print(find_similarities(entity_embedding, 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##RecSys\n"
      ],
      "metadata": {
        "id": "4SMCn5Iz0aqg"
      },
      "id": "4SMCn5Iz0aqg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The NER extracts a list of movies that the user likes and appends them to a list. Based on the liked movies, the RecSys will generate recommendations by leveraging both the embeddings of these movies and their genres. First, it checks if each liked movie exists in a knowledge graph. If a movie isn't found, it attempts to find the closest matching entity using the editdistance function. The embeddings for the liked movies (or their closest matches) are then averaged to create a representative vector.\n",
        "\n",
        "Using cosine similarity, it calculates how similar this average embedding is to all other movie embeddings in the graph. It sorts these similarities to identify the most similar movies while excluding any that are already in the question or their closest matches. Finally, it returns a specified number of unique movie recommendations that are not part of the user's liked list."
      ],
      "metadata": {
        "id": "54vKgLLFtlAc"
      },
      "id": "54vKgLLFtlAc"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def recommend_movies_by_genre(liked_movies, n=5):\n",
        "    \"\"\"\n",
        "    Recommends movies similar to the user's liked movies based on embeddings and genre.\n",
        "    Uses closest matching entity if the exact movie is not in the knowledge graph.\n",
        "\n",
        "    Args:\n",
        "        liked_movies (list): List of movie labels liked by the user.\n",
        "        n (int): Number of recommendations to generate.\n",
        "\n",
        "    Returns:\n",
        "        list: List of recommended movie labels.\n",
        "    \"\"\"\n",
        "    liked_movies_embeddings = []\n",
        "    for movie in liked_movies:\n",
        "        #print(f\"Processing movie: {movie}\")\n",
        "        if movie in ent2uri:  # If the movie is in the graph, use its embedding directly\n",
        "            liked_movies_embeddings.append(extract_embedding(movie))\n",
        "            #print(liked_movies_embeddings)\n",
        "\n",
        "\n",
        "        else:  # If not, find the closest matching entity\n",
        "            threshold = max(3, int(len(movie) * 0.9))  # Adjust threshold\n",
        "            match = match_entity_editdistance(movie, threshold=threshold)\n",
        "            if match:\n",
        "                match_node, match_value, _ = match\n",
        "                # Check if the match_value is a valid key before proceeding\n",
        "                if match_value in ent2uri:\n",
        "                    liked_movies_embeddings.append(extract_embedding(match_value))\n",
        "                    print(f\"Using '{match_value}' instead of '{movie}' for recommendation.\")\n",
        "                else:\n",
        "                    print(f\"Closest match '{match_value}' not found in embedding data. Skipping.\")\n",
        "            else:\n",
        "                print(f\"Could not find a close match for '{movie}' in the knowledge graph.\")\n",
        "\n",
        "    if not liked_movies_embeddings:\n",
        "        return \"None of the provided movies were found in the knowledge graph.\"\n",
        "\n",
        "    # Calculate the average embedding of liked movies (or their closest matches)\n",
        "    #print(f\"Liked movies embeddings: {liked_movies_embeddings}\")\n",
        "    avg_embedding = np.mean(liked_movies_embeddings, axis=0)\n",
        "\n",
        "    # Calculate similarity to all other movie embeddings\n",
        "    similarities = cosine_similarity(avg_embedding.reshape(1, -1), entity_matrix)\n",
        "\n",
        "    # Get indices of most similar movies (excluding liked movies and their close matches)\n",
        "    sorted_indices = similarities.argsort()[0][::-1]  # Sort in descending order\n",
        "\n",
        "    excluded_movies = set(liked_movies + [match_value for _, match_value, _ in [match_entity_editdistance(m) for m in liked_movies if match_entity_editdistance(m)]if match_value])\n",
        "\n",
        "    # Filter recommended indices, excluding liked movies and their close matches\n",
        "    # The change is here: Add a check if id2ent[i] is in nodes before accessing it\n",
        "    recommended_indices = [i for i in sorted_indices if id2ent[i] in nodes and nodes[id2ent[i]] not in excluded_movies]\n",
        "\n",
        "    # Return labels of recommended movies\n",
        "    recommendations = [nodes[id2ent[i]] for i in recommended_indices[:n]]\n",
        "    return recommendations"
      ],
      "metadata": {
        "id": "H88pbE3w0dok"
      },
      "id": "H88pbE3w0dok",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Crowdsource System\n",
        "\n",
        "\n",
        "**Crowdsourcing Workflow for Knowledge Graph Integration**\n",
        "\n",
        "The crowdsourcing process validates and augments RDF triples in the knowledge graph by leveraging responses in the crowd_data.tsv.\n",
        "\n",
        "**Filtering Malicious Workers:**\n",
        "Workers with a LifetimeApprovalRate below 50% or WorkTimeInSeconds below 35 seconds are excluded. For each microtask (HITId), the top 3 valid workers (ranked by time spent) are retained.\n",
        "\n",
        "**Answer Aggregation:**\n",
        "Responses are aggregated via majority voting:\n",
        "\n",
        "CORRECT votes use the value from Input3ID as the answer.\n",
        "INCORRECT votes use the value from FixValue.\n",
        "\n",
        "**Inter-Rater Agreement:**\n",
        "Fleiss' Kappa is calculated for each batch (HITTypeId) to assess response reliability. This metric is shared in the output.\n",
        "\n",
        "**Graph Integration:**\n",
        " New entities and predicates are added to the graph if not already present. Triples are inserted in RDF format, and literals are appropriately handled.\n",
        "\n",
        "Entity Label Retrieval: **bold text**\n",
        "Entity IDs (e.g., wd:Q27096213) are resolved to human-readable labels using the graph."
      ],
      "metadata": {
        "id": "Xxefu3gg58Dw"
      },
      "id": "Xxefu3gg58Dw"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from statsmodels.stats.inter_rater import fleiss_kappa\n",
        "\n",
        "\n",
        "# Load the crowdsourced data\n",
        "data = pd.read_csv('/content/drive/MyDrive/Speakeasy_Project/Speakeasy Project/Datasets/crowd_data.tsv', sep='\\t')\n",
        "\n",
        "# Convert 'LifetimeApprovalRate' from percentage string to float\n",
        "data['LifetimeApprovalRate'] = data['LifetimeApprovalRate'].replace('%', '', regex=True).astype(float)\n",
        "\n",
        "# Convert 'WorkTimeInSeconds' to numeric values, forcing errors to NaN\n",
        "data['WorkTimeInSeconds'] = pd.to_numeric(data['WorkTimeInSeconds'], errors='coerce')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def filter_workers(data, approval_threshold=50, time_threshold=35):\n",
        "    \"\"\"\n",
        "    Filters out malicious workers based on LifetimeApprovalRate and WorkTimeInSeconds.\n",
        "    Returns a filtered DataFrame named valid_workers.\n",
        "    removes all the workers who have a LifetimeApprovalRate less than 50 and WorkTimeInSeconds less than 35 seconds.\n",
        "\n",
        "    \"\"\"\n",
        "    print(\"\\nInitial dataset size:\", data.shape)\n",
        "\n",
        "    # Filter workers based on LifetimeApprovalRate and WorkTimeInSeconds\n",
        "    valid_workers = data[(data['LifetimeApprovalRate'] >= approval_threshold) & (data['WorkTimeInSeconds'] >= time_threshold)]\n",
        "\n",
        "    #print(f\"Filtered dataset size after approval rate >= {approval_threshold} and time >= {time_threshold}: {valid_workers.shape}\")\n",
        "\n",
        "    # Sort the filtered data and select top 3 raters per HIT\n",
        "    valid_workers = valid_workers.sort_values(by='WorkTimeInSeconds', ascending=False)\n",
        "\n",
        "    # Select top 3 raters for each HITId\n",
        "    valid_workers = valid_workers.groupby('HITId').head(3)\n",
        "\n",
        "    #print(f\"Dataset size after selecting top 3 raters per HITId: {valid_workers.shape}\")\n",
        "\n",
        "    return valid_workers\n",
        "\n",
        "def merge_crowdsourced_data(graph, data, ent2id, pred2id, id2ent, id2pred):\n",
        "    \"\"\"\n",
        "    Merges crowdsourced data into the knowledge graph, adding new entities and relations.\n",
        "    Checks if the entity or relation already exists in the graph.\n",
        "    Adds the graph with the entities and relations which are not already present.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Iterate through the valid crowdsourced data\n",
        "    for index, row in valid_workers.iterrows():\n",
        "        subject_id = row['Input1ID']\n",
        "        predicate_id = row['Input2ID']\n",
        "        object_id = row['Input3ID']\n",
        "\n",
        "        print(f\"Processing triple: ({subject_id}, {predicate_id}, {object_id})\")\n",
        "\n",
        "        # Check if entities and predicates exist in the graph\n",
        "        if subject_id not in ent2id:\n",
        "            # Add new entity to the graph and dictionaries\n",
        "            ent2id[subject_id] = len(ent2id)\n",
        "            id2ent[len(id2ent)] = subject_id\n",
        "            graph.add((rdflib.URIRef(subject_id), rdflib.RDF.type, rdflib.URIRef('http://www.wikidata.org/entity/Q35120')))\n",
        "            print(f\"    Added new entity: {subject_id}\")\n",
        "\n",
        "        else:\n",
        "            print(f\"    Entity {subject_id} already exists.\")\n",
        "\n",
        "        if predicate_id not in pred2id:\n",
        "            # Add new predicate to the graph and dictionaries\n",
        "            pred2id[predicate_id] = len(pred2id)\n",
        "            id2pred[len(id2pred)] = predicate_id\n",
        "            graph.add((rdflib.URIRef(predicate_id), rdflib.RDF.type, rdflib.URIRef('http://www.wikidata.org/prop/direct/')))\n",
        "            print(f\"    Added new predicate: {predicate_id}\")\n",
        "\n",
        "        else:\n",
        "            print(f\"    Predicate {predicate_id} already exists.\")\n",
        "\n",
        "\n",
        "\n",
        "        if object_id not in ent2id and object_id.startswith('wd:'):  # Only add if new and entity-like\n",
        "            # Add new entity (object) to the graph and dictionaries\n",
        "            ent2id[object_id] = len(ent2id)\n",
        "            id2ent[len(id2ent)] = object_id\n",
        "            graph.add((rdflib.URIRef(object_id), rdflib.RDF.type, rdflib.URIRef('http://www.wikidata.org/entity/Q35120')))\n",
        "            print(f\"    Added new object entity: {object_id}\")\n",
        "\n",
        "\n",
        "        elif object_id not in ent2id:\n",
        "            print(f\"    Object is a literal: {object_id}\")  # Indicate if it's a literal\n",
        "\n",
        "        else:\n",
        "            print(f\"    Object entity {object_id} already exists.\")\n",
        "\n",
        "        # Add the triple to the graph\n",
        "        subject_uri = rdflib.URIRef(subject_id)\n",
        "        predicate_uri = rdflib.URIRef(predicate_id)\n",
        "\n",
        "        if object_id.startswith('wd:'):\n",
        "            object_uri = rdflib.URIRef(object_id)\n",
        "\n",
        "        else:\n",
        "            object_uri = rdflib.Literal(object_id)\n",
        "\n",
        "        graph.add((subject_uri, predicate_uri, object_uri))\n",
        "        print(f\"    Added triple to the graph.\")\n",
        "\n",
        "\n",
        "def find_entity_in_crowd_data(extracted_entity_label, crowd_data):\n",
        "    \"\"\"\n",
        "    Searches for the extracted entity in the crowdsourced data.\n",
        "    Returns the entity ID, relation ID, and object ID if found.\n",
        "    if not, then it returns None.\n",
        "    \"\"\"\n",
        "\n",
        "    # Direct lookup for exact match (if the label is already an entity ID)\n",
        "    entity_id = crowd_data.loc[crowd_data['Input1ID'] == extracted_entity_label, 'Input1ID'].iloc[0] if crowd_data[crowd_data['Input1ID'] == extracted_entity_label].shape[0] > 0 else None\n",
        "    if entity_id:\n",
        "\n",
        "        relation_id = crowd_data.loc[crowd_data['Input1ID'] == extracted_entity_label, 'Input2ID'].iloc[0]\n",
        "        object_id = crowd_data.loc[crowd_data['Input1ID'] == extracted_entity_label, 'Input3ID'].iloc[0]\n",
        "\n",
        "        return entity_id, relation_id, object_id\n",
        "\n",
        "    # If not found directly, try matching with nodes in the graph\n",
        "    print(f\"Direct lookup for extracted_entity_label: '{extracted_entity_label}': {entity_id}\")\n",
        "    match = match_entity_editdistance(extracted_entity_label, nodes)\n",
        "    print(f\"Match for '{extracted_entity_label}': {match}\")\n",
        "\n",
        "    if match:\n",
        "        matched_node, _, _ = match\n",
        "\n",
        "        # Convert matched_node to the format used in crowd_data (e.g., wd:Q1410031)\n",
        "        matched_node_id = \"wd:\" + matched_node.split('/')[-1]  # Extract the entity ID part\n",
        "        print(f\"matched_node_id: {matched_node_id}\")\n",
        "\n",
        "        # Search for the converted ID in crowd_data\n",
        "        entity_id = crowd_data.loc[crowd_data['Input1ID'] == matched_node_id, 'Input1ID'].iloc[0] if crowd_data[crowd_data['Input1ID'] == matched_node_id].shape[0] > 0 else None\n",
        "\n",
        "\n",
        "        if entity_id:\n",
        "            relation_id = crowd_data.loc[crowd_data['Input1ID'] == matched_node_id, 'Input2ID'].iloc[0]\n",
        "            object_id = crowd_data.loc[crowd_data['Input1ID'] == matched_node_id, 'Input3ID'].iloc[0]\n",
        "\n",
        "            return entity_id, relation_id, object_id\n",
        "\n",
        "    # If still not found, return None\n",
        "    return None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def calculate_fleiss_kappa(valid_workers):\n",
        "\n",
        "    \"\"\"\n",
        "    Calculate Fleiss' Kappa value for the batch based on 'HITId' and 'AnswerLabel'.\n",
        "    Returns the Fleiss' Kappa value.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    if len(valid_workers['AnswerLabel'].unique()) < 2:\n",
        "        print(\"Not enough variety in answers (only one unique answer). Returning 0.\")\n",
        "\n",
        "        return 0  # If all answers are the same (e.g., no variation), return 0\n",
        "\n",
        "    # Create a contingency table (HITId vs. AnswerLabel)\n",
        "    contingency_table = pd.crosstab(valid_workers['HITId'], valid_workers['AnswerLabel'])\n",
        "\n",
        "    # Check for empty contingency table (no data for this entity/relation)\n",
        "    if contingency_table.empty:\n",
        "        print(\"Contingency table is empty. Returning -1 (indicating no data).\")\n",
        "\n",
        "        return -1\n",
        "\n",
        "    # Calculate Fleiss' Kappa using the statsmodels function\n",
        "    try:\n",
        "        kappa = fleiss_kappa(contingency_table.to_numpy())\n",
        "        print(f\"Fleiss' Kappa calculated: {kappa}\")\n",
        "\n",
        "        return round(kappa, 3)\n",
        "\n",
        "    except ValueError:\n",
        "        print(\"Error calculating Fleiss' Kappa (likely due to insufficient data). Returning -1.\")\n",
        "\n",
        "        return -1\n",
        "\n",
        "\n",
        "\n",
        "def majority_vote(valid_workers):\n",
        "\n",
        "    \"\"\"\n",
        "    Perform majority voting to get the final answer.\n",
        "    Returns the final answer, correct count, and incorrect count.\n",
        "    \"\"\"\n",
        "    # Aggregate the answers: count the \"CORRECT\" vs \"INCORRECT\" responses\n",
        "    correct_count = (valid_workers['AnswerLabel'] == 'CORRECT').sum()\n",
        "    incorrect_count = (valid_workers['AnswerLabel'] == 'INCORRECT').sum()\n",
        "\n",
        "    # Debug print to verify the counts for majority voting\n",
        "    print(f\"\\nMajority vote counts: CORRECT = {correct_count}, INCORRECT = {incorrect_count}\")\n",
        "\n",
        "    # The final answer is based on the majority\n",
        "    if correct_count >= incorrect_count:\n",
        "        final_answer = \"CORRECT\"\n",
        "\n",
        "    else:\n",
        "        final_answer = \"INCORRECT\"\n",
        "\n",
        "    return final_answer, correct_count, incorrect_count\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_entity_label(entity_id):\n",
        "\n",
        "    \"\"\"\n",
        "    This function is later used to fetch the entity from the graph if the answer found in the crowd_data is structured as a ID.\n",
        "    Retrieves the label of a Wikidata entity given its ID.\n",
        "    Returns the label as a string.\n",
        "    \"\"\"\n",
        "    graph = Graph()\n",
        "    graph.parse(f\"https://www.wikidata.org/wiki/Special:EntityData/{entity_id[3:]}.ttl\", format=\"turtle\")\n",
        "    entity_uri = URIRef(f\"http://www.wikidata.org/entity/{entity_id[3:]}\")\n",
        "    for label in graph.objects(entity_uri, URIRef(\"http://schema.org/name\")):\n",
        "\n",
        "        return label.value\n",
        "\n",
        "    return \"Label not found\"\n",
        "\n",
        "\n",
        "#some patch ups done to the calculation of Fleiss kappa.\n",
        "contingency_tables = valid_workers.groupby(['HITTypeId', 'HITId'])['AnswerLabel'].value_counts().unstack(fill_value=0)\n",
        "fleiss_kappa_per_batch = contingency_tables.groupby(level=0).apply(fleiss_kappa)\n",
        "print(fleiss_kappa_per_batch)\n",
        "\n",
        "\n",
        "\n",
        "def crowdsource_search_with_valid_workers(entity, relation, valid_workers):\n",
        "\n",
        "    \"\"\"\n",
        "    Search the crowdsourced data for matching triples, filtered by valid workers.\n",
        "    If the majority of the responses of the valid_workers are 'CORRECT' then it will consider the value in 'Input3ID' for answer computation.\n",
        "    If the majority of the responses are 'INCORRECT' then it will consider the value in 'FixValue' for answer computation.\n",
        "    If the answer is structured as an ID, then it will use the get_entity_label function to convert it into its corresponding entity.\n",
        "    Returns the answer as a string.\n",
        "\n",
        "    \"\"\"\n",
        "    # Filter data based on valid workers and entity, relation\n",
        "    res = valid_workers[(valid_workers['Input1ID'] == entity) & (valid_workers['Input2ID'] == relation)]\n",
        "\n",
        "    print(f\"\\nFiltered results for entity: {entity} and relation: {relation}:\")\n",
        "    #print(res.head())  # Print a sample of the filtered results to check for any valid answers\n",
        "\n",
        "    if res.empty:\n",
        "\n",
        "        return \"No valid crowdsourced answers found for this question.\"\n",
        "\n",
        "    # Get Fleiss' Kappa for the batch (HITTypeId)\n",
        "    hit_type_id = res['HITTypeId'].iloc[0]  # Get HITTypeId from the filtered results\n",
        "    kappa = fleiss_kappa_per_batch.get(hit_type_id, 0) # Get Kappa for the batch, default to 0 if not found\n",
        "    kappa = round(kappa, 3)\n",
        "\n",
        "    # Perform majority voting\n",
        "    final_answer, correct_count, incorrect_count = majority_vote(res)\n",
        "\n",
        "    # Extract the answer based on majority vote\n",
        "    if final_answer == \"CORRECT\":\n",
        "        answer = res['Input3ID'].unique()  # Fetch from Input3ID if CORRECT\n",
        "\n",
        "    else:\n",
        "        answer = res['FixValue'].unique()  # Fetch from FixValue if INCORRECT\n",
        "\n",
        "    # Convert ID-like answers to entity labels\n",
        "    converted_answer = []\n",
        "    for ans in answer:\n",
        "        if ans.startswith(\"wd:\"):\n",
        "            # Use your get_entity_label function to fetch the label\n",
        "            entity_label = get_entity_label(ans)\n",
        "            converted_answer.append(entity_label)\n",
        "\n",
        "        else:\n",
        "            converted_answer.append(ans)\n",
        "\n",
        "\n",
        "    #print(f\"\\nFinal answer: {converted_answer}, Fleiss' Kappa: {kappa}\")  # Use converted_answer here\n",
        "    return f\"The answers are {converted_answer}. [Crowd, inter-rater agreement {kappa}, The answer distribution for this specific task was {correct_count} support votes, {incorrect_count} reject votes]\"  # Use converted_answer here\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jCoRrEdOuXT",
        "outputId": "5f874a76-ce6d-4db6-961d-22981c7ab810"
      },
      "id": "9jCoRrEdOuXT",
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HITTypeId\n",
            "7QT    0.236364\n",
            "8QT    0.040000\n",
            "9QT    0.199110\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#sentence = \"What is the box office of The Princess and the Frog?\"\n",
        "#sentence = 'Who is the executive producer of X-Men: First Class?'\n",
        "sentence = 'Can you tell me the publication date of Tom Meets Zizou?'\n",
        "extracted_entities, exit_status, all_extracted_entities = extract_entities_NER(sentence, predicates, n=2, confidence=0.6)\n",
        "print(f'all_extracted_entities: {all_extracted_entities}')\n",
        "print(f'sentence: {sentence}')\n",
        "\n",
        "#Extract entity and relation from all_extracted_entities\n",
        "entity_label = all_extracted_entities[0]\n",
        "relation_label = all_extracted_entities[1]\n",
        "\n",
        "# Search for the entity in crowd data\n",
        "result = find_entity_in_crowd_data(entity_label, data)\n",
        "\n",
        "if result:\n",
        "    entity_id, relation_id, object_id = result\n",
        "    #print(f\"Entity ID: {entity_id}\")\n",
        "    #print(f\"Relation ID: {relation_id}\")\n",
        "    #print(f\"Object ID: {object_id}\")\n",
        "\n",
        "    # Search for the answer in crowd data using valid workers\n",
        "    valid_workers = filter_workers(data)\n",
        "    answer = crowdsource_search_with_valid_workers(entity_id, relation_id, valid_workers)\n",
        "    print(answer)\n",
        "\n",
        "    # If the entity or relation is not in the knowledge graph, add it\n",
        "    if entity_id not in ent2id:\n",
        "        merge_crowdsourced_data(graph, data, ent2id, pred2id, id2ent, id2pred)  # Merge data into the graph\n",
        "        #print(f\"Entity '{entity_id}' added to knowledge graph.\")\n",
        "\n",
        "    if relation_id not in pred2id:\n",
        "        merge_crowdsourced_data(graph, data, ent2id, pred2id, id2ent, id2pred)  # Merge data into the graph\n",
        "        #print(f\"Relation '{relation_id}' added to knowledge graph.\")\n",
        "else:\n",
        "    print(\"Entity not found in crowd data.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBBWYhMUOQ21",
        "outputId": "cf6d82d6-41f7-42a6-8ba8-c53a9f6f63a8"
      },
      "id": "WBBWYhMUOQ21",
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all_extracted_entities: ['Tom Meets Zizou', 'publication date']\n",
            "sentence: Can you tell me the publication date of Tom Meets Zizou?\n",
            "Direct lookup for extracted_entity_label: 'Tom Meets Zizou': None\n",
            "Match for 'Tom Meets Zizou': ('http://www.wikidata.org/entity/Q1410031', 'Tom Meets Zizou', 0)\n",
            "matched_node_id: wd:Q1410031\n",
            "\n",
            "Initial dataset size: (305, 16)\n",
            "\n",
            "Filtered results for entity: wd:Q1410031 and relation: wdt:P577:\n",
            "\n",
            "Majority vote counts: CORRECT = 0, INCORRECT = 3\n",
            "The answers are ['2011-01-01']. [Crowd, inter-rater agreement 0.04, The answer distribution for this specific task was 0 support votes, 3 reject votes]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e9625da-eeb8-4b7c-8394-0360f83a77e8",
      "metadata": {
        "id": "0e9625da-eeb8-4b7c-8394-0360f83a77e8"
      },
      "source": [
        "# Answer questions with Embeddings\n",
        "\n",
        "We can now use the following pipeline for answering questions:\n",
        "- Extract the entities and relation from the question\n",
        "- Turn entities and relation into embeddings\n",
        "- If the entity is a subject, retrieve the object by: _object = subject + relation_\n",
        "- If the entity is an object, retrieve the subject by _subject = object - relation_"
      ]
    },
    {
      "source": [
        "def answer_question_embeddings(question):\n",
        "    entities, exit_status = extract_entities_NER(question, predicates, n=3, confidence=0.6)\n",
        "    recommendation_keywords = [\"Recommend\", \"recommend\", \"Recommendation\", \"recommendation\", \"Suggest\", \"suggest\", \"Suggestion\", \"suggestion\", \"like\"]\n",
        "    liked_movies = []\n",
        "\n",
        "    # Handle cases based on exit_status\n",
        "    if exit_status == 'No entities found by NER':\n",
        "        return \"We could not find any entities in the question. Could you verify that you have capitalized the right letters, such as movie titles or people’s names?\"\n",
        "\n",
        "    elif exit_status == 'Entity matching too distant':\n",
        "        #match_value, _ = entities  # entities contains the match value in this case\n",
        "        #return f\"The closest entity match found was '{match_value}', but it seems too distant. Could you rephrase it or specify it more clearly?\"\n",
        "        # Check if entities is a tuple with more than 2 elements\n",
        "        if isinstance(entities, tuple) and len(entities) > 2:\n",
        "            # If so, assume the first element is the match value\n",
        "            match_value = entities[0]\n",
        "        else:\n",
        "            # Otherwise, unpack as before if there are only two elements\n",
        "            try:\n",
        "                match_value, _ = entities\n",
        "            except (TypeError, ValueError):\n",
        "                # Handle cases where entities is not iterable or has unexpected format\n",
        "                return \"Error: Unexpected format for entities. Please check the extract_entities_NER function.\"\n",
        "        return f\"The closest entity match found was '{match_value}', but it seems too distant. Could you rephrase it or specify it more clearly?\"\n",
        "\n",
        "\n",
        "    elif exit_status == 'predicate missing embedding':\n",
        "        relation, _ = entities  # entities contains the relation in this case\n",
        "        return f\"Unfortunately, we were not provided with an embedding for the relation '{relation}'. Please try another question.\"\n",
        "\n",
        "    elif any(keyword in question for keyword in recommendation_keywords):\n",
        "        # Assuming extract_entities_NER returns a tuple of (match_value, liked_movies)\n",
        "        # for recommendation-type queries\n",
        "        liked_movies = entities\n",
        "        # If entities contains more than two elements, assume the liked movies\n",
        "        # are stored in the second element\n",
        "\n",
        "        return recommend_movies_by_genre(liked_movies)\n",
        "    # Proceed if everything worked correctly\n",
        "    # Check if entities is a list of dictionaries before proceeding\n",
        "    if not exit_status and isinstance(entities, list) and all(isinstance(item, dict) for item in entities):\n",
        "        extracted_predicates = [d['word'] for d in entities if d['entity_group'] == 'predicate']\n",
        "\n",
        "        # Check if extracted_predicates is empty\n",
        "        if extracted_predicates:\n",
        "            extracted_predicates = extracted_predicates[0]  # Access the first element only if it exists\n",
        "        else:\n",
        "            # Handle the case where no predicates are found (e.g., return an error message or a default value)\n",
        "            return \"No predicate found in the question.\"  # Or handle it differently\n",
        "\n",
        "        extracted_entities = [d['word'] for d in entities if d['entity_group'] != 'predicate']\n",
        "        # Extract predicates and entities\n",
        "        #extracted_predicates = [d['word'] for d in entities if d['entity_group'] == 'predicate'][0]\n",
        "        #extracted_entities = [d['word'] for d in entities if d['entity_group'] != 'predicate']\n",
        "\n",
        "        # Convert predicates and entities to embeddings\n",
        "        predicates_embeddings = [extract_embedding(pred, 'predicate') for pred in extracted_predicates]\n",
        "        entities_embeddings = [extract_embedding(ent) for ent in extracted_entities]\n",
        "\n",
        "        # Compute answer using similarity function\n",
        "        answer = find_similarities(entities_embeddings[0] + predicates_embeddings[0], 3)\n",
        "        return answer\n",
        "    else:\n",
        "        return \"Error: Invalid format for entities. Please check the extract_entities_NER function.\""
      ],
      "cell_type": "code",
      "metadata": {
        "id": "thiqndfUBsl9"
      },
      "id": "thiqndfUBsl9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1251c5d7-4008-429e-a518-3a4ef51645b2",
      "metadata": {
        "id": "1251c5d7-4008-429e-a518-3a4ef51645b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c01fbb23-6d37-451b-87d0-d552c3657d2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question after preprocessing: Who is the director of Star Wars Episode VI Return of the Jedi?\n",
            "\n",
            "Extracted entity: Star Wars Episode VI Return of the Jedi\n",
            "\n",
            "Question after removing entities: who is the director of ?\n",
            "\n",
            "['George Lucas', 'Anthony Daniels', 'Ellis Rubin']\n"
          ]
        }
      ],
      "source": [
        "# Example Usage\n",
        "\n",
        "question = \"Who is the director of Star Wars: Episode VI - Return of the Jedi?\"\n",
        "\n",
        "print(answer_question_embeddings(question))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question1 = \"Given that I like Inception and Jumanji, can you recommend some movies?\"\n",
        "\n",
        "print(answer_question_embeddings(question1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2faceb37-059d-443d-c7f0-4fe9998d06f0",
        "id": "rmoMWAz4sg-A"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question after preprocessing: Given that I like Inception and Jumanji, can you recommend some movies?\n",
            "\n",
            "Extracted entity: Inception\n",
            "\n",
            "Extracted entity: Jumanji\n",
            "\n",
            "Question after removing entities: given that i like and , can you recommend some movies?\n",
            "\n",
            "Liked movies: ['Inception', 'Jumanji']\n",
            "\n",
            "['Jumanji: Welcome to the Jungle', 'Jumanji: The Next Level', 'Big Fish', 'Sherlock Holmes', 'Iron Man 3']\n"
          ]
        }
      ],
      "id": "rmoMWAz4sg-A"
    },
    {
      "cell_type": "code",
      "source": [
        "question2 = \"Recommend movies like Nightmare on Elm Street, Friday the 13th, and Halloween.\"\n",
        "\n",
        "print(answer_question_embeddings(question2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "616663a5-a88a-4e49-98ad-04a6b8b3354f",
        "id": "SOzggo3isg-G"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question after preprocessing: Recommend movies like Nightmare on Elm Street, Friday the 13th, and Halloween.\n",
            "\n",
            "Extracted entity: Nightmare on Elm Street\n",
            "\n",
            "Extracted entity: Friday the 13th\n",
            "\n",
            "Extracted entity: Halloween\n",
            "\n",
            "Question after removing entities: recommend movies like , , and .\n",
            "\n",
            "Liked movies: ['A Nightmare on Elm Street', 'Friday the 13th', 'Halloween']\n",
            "\n",
            "['A Nightmare on Elm Street 3: Dream Warriors', 'A Nightmare on Elm Street 4: The Dream Master', \"Freddy's Dead: The Final Nightmare\", 'A Nightmare on Elm Street 5: The Dream Child', 'Never Sleep Again: The Elm Street Legacy']\n"
          ]
        }
      ],
      "id": "SOzggo3isg-G"
    },
    {
      "cell_type": "code",
      "source": [
        "question3 = \"Given that I like Madagascar 1, Pocahontas, and Rio, can you recommend some movies? \"\n",
        "\n",
        "print(answer_question_embeddings(question3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf42ff22-1d95-4ec2-da61-7ba5568a53f4",
        "id": "HdAHsZ-Csg-H"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question after preprocessing: Given that I like Madagascar 1, Pocahontas, and Rio, can you recommend some movies?\n",
            "\n",
            "Extracted entity: Madagascar 1\n",
            "\n",
            "Extracted entity: Pocahontas\n",
            "\n",
            "Extracted entity: Rio\n",
            "\n",
            "Question after removing entities: given that i like , , and , can you recommend some movies?\n",
            "\n",
            "Liked movies: ['Madagascar', 'Pocahontas', 'Rio']\n",
            "\n",
            "['Robots', 'Animals United', 'Ice Age: Dawn of the Dinosaurs', 'Ice Age: The Meltdown', 'Rio 2']\n"
          ]
        }
      ],
      "id": "HdAHsZ-Csg-H"
    },
    {
      "cell_type": "code",
      "source": [
        "question6 = \"Given that I like The Lion King, Pocahontas, and The Beauty and the Beast, can you recommend some movies?\"\n",
        "\n",
        "print(answer_question_embeddings(question6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f761895e-b53b-4112-ef05-fda1d81e9470",
        "id": "q9W-aVbDsg-H"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question after preprocessing: Given that I like The Lion King, Pocahontas, and The Beauty and the Beast, can you recommend some movies?\n",
            "\n",
            "Extracted entity: The Lion King\n",
            "\n",
            "Extracted entity: Pocahontas\n",
            "\n",
            "Extracted entity: The Beauty and the Beast\n",
            "\n",
            "Question after removing entities: given that i like , , and , can you recommend some movies?\n",
            "\n",
            "Liked movies: ['The Lion King', 'Pocahontas', 'Beauty and the Beast']\n",
            "\n",
            "['Aladdin', 'The Little Mermaid', 'The Hunchback of Notre Dame', 'Oliver & Company', 'The Rescuers Down Under']\n"
          ]
        }
      ],
      "id": "q9W-aVbDsg-H"
    },
    {
      "cell_type": "markdown",
      "id": "cfd239b3-664e-4461-9dd9-940b4bb3f463",
      "metadata": {
        "id": "cfd239b3-664e-4461-9dd9-940b4bb3f463"
      },
      "source": [
        "# TODO\n",
        "\n",
        "- Implement way to handle double questions like \"who is the director of ... AND who is the screenwriter of ....\"\n",
        "- Finish and perfect factual questions queries\n",
        "- Implement language model to generate more realistic responses"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c1537d3-888a-48ea-8c0b-b373fccde4d2",
      "metadata": {
        "id": "7c1537d3-888a-48ea-8c0b-b373fccde4d2"
      },
      "source": [
        "## Factual question answering\n",
        "\n",
        "For factual question we will proceed in the following way:\n",
        "- Write a list of common question patterns using re to extract relations and entities from them\n",
        "- Extract the probable relations and entities from the question matching it to the pattern and map those probable entities to actual entities in the graph using edit distance or embedding similarity with spacy\n",
        "- Generate a custom sparql query based on the question pattern matched\n",
        "- Query the graph with the custom query"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0c60ef5-cb24-4560-a599-31506aa8041a",
      "metadata": {
        "id": "e0c60ef5-cb24-4560-a599-31506aa8041a"
      },
      "source": [
        "### Step 1: Write a list of question patterns\n",
        "The question pattern contains:\n",
        "- The actual pattern like r\"(?:find|which) movies.*contain(?:s)?(?: the word)? (?P<word>\\w+)\"\n",
        "- A string associated to that specific question type which we are going to use to map question type to custom queries\n",
        "- A boolean value that is 1 if we want to match the entities from the question to actual entities in the graph and 0 if we don't. One example could be question type: r\"movies rated below (?P<number>\\d+(\\.\\d+)?)\" which asks the chatbot to list all the movies rated below a certain score. In this case there are no entities to be retrieved from the graph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9435373-5dcd-4337-9772-e92cb0d73e15",
      "metadata": {
        "id": "d9435373-5dcd-4337-9772-e92cb0d73e15"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "question_patterns = [\n",
        "\n",
        "    # Pattern 0: who and what\n",
        "    (r\"who is the (?P<relation>.+?) of (?P<entity>.+)\", 'who', 1),\n",
        "\n",
        "    # Pattern 1: Find movies with (word) in their titles\n",
        "    (r\"(?:find|which) movies.*contain(?:s)?(?: the word)? (?P<word>\\w+)\", 'find_word_in_title', 0),\n",
        "    (r\"(?:find|which) movies with (?P<word>\\w+) in (?:their )?titles?\", 'find_word_in_title', 0),\n",
        "    (r\"(?:find|which) movies (?:whose )?(?:title|name) contains? (?P<word>\\w+)\", 'find_word_in_title', 0),\n",
        "\n",
        "    # Pattern 2: Highest-rated movies (optional 'above' and a number)\n",
        "    (r\"(?:what are|list)(?: the)?(?: highest[-\\s]rated)? movies(?: rated)?(?: above| greater than)?(?: (?P<number>\\d+(\\.\\d+)?))?\", 'movies_rating_above', 0),\n",
        "    (r\"movies (?:rated )?(?:above )?(?P<number>\\d+(\\.\\d+)?)?\", 'movies_rating_above', 0),\n",
        "\n",
        "    # Pattern 3: Lowest-rated movies (optional 'below' and a number)\n",
        "    (r\"(?:what are|list)(?: the)?(?: lowest[-\\s]rated)? movies(?: rated)?(?: below| less than)?(?: (?P<number>\\d+(\\.\\d+)?))?\", 'movies_rating_below', 0),\n",
        "    (r\"movies (?:rated )?(?:below )?(?P<number>\\d+(\\.\\d+)?)?\", 'movies_rating_below', 0),\n",
        "\n",
        "    # Pattern 4: Entities in alphabetical order\n",
        "    (r\"which (?P<entity>.+) comes first alphabetically\", 'entity_first_alphabetically', 1),\n",
        "    (r\"list (?P<entity>.+) in alphabetical order\", 'entity_first_alphabetically', 1),\n",
        "\n",
        "    # Pattern 5: Entities in reverse alphabetical order\n",
        "    (r\"which (?P<entity>.+) comes last alphabetically\", 'entity_last_alphabetically', 1),\n",
        "    (r\"list (?P<entity>.+) in reverse alphabetical order\", 'entity_last_alphabetically', 1),\n",
        "\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d6ce000-8648-48d9-a541-65dfe2011b24",
      "metadata": {
        "id": "2d6ce000-8648-48d9-a541-65dfe2011b24"
      },
      "source": [
        "### Step 2: Process question to extract entities\n",
        "\n",
        "We will proceed in the following way:\n",
        "\n",
        "- Extract the dictionary of matched entities via the .groupdict() method.\n",
        "    - For question r\"who is the director of Star Wars\" matched to pattern r\"who is the (?P<relation>.+?) of (?P<entity>.+)\" the dictionary looks like this {'relation': 'director', 'entity': 'Star Wars'}\n",
        "- Append the question type to the dictionary. So for r\"who is the director of Star Wars\" it would result in: {'relation': 'director', 'entity': 'Star Wars', 'qtype': 'who'}\n",
        "- If the question type needs to match entities to the knowledge graph (boolean value == 1) then we gather the relation and/or entity that we extracted from the question via matching the pattern and we match those values to actual entities in the graph\n",
        "    - For entities we try to match them with the match_entity_editdistance function.\n",
        "    - For predicates we need to account for synonyms so we use the check_ngram_match function that takes a potential predicate and tries to match it via editDistance to an actual predicate and if it doesn't work it tries by evaluating embeddings similarity via spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cc83579-8238-4751-9d25-0c537c7438f7",
      "metadata": {
        "id": "4cc83579-8238-4751-9d25-0c537c7438f7"
      },
      "outputs": [],
      "source": [
        "def process_question_factual(question, entity_dictionary, predicate_dictionary):\n",
        "\n",
        "    for pattern, qtype, matching in question_patterns:\n",
        "\n",
        "        print(f\"pattern = {pattern}\")\n",
        "        match = re.match(pattern, question, re.IGNORECASE)\n",
        "\n",
        "        if match:\n",
        "            params = match.groupdict()\n",
        "            params['type'] = qtype  # Add the question type to the params\n",
        "            print(f\"Question matched to pattern {qtype}\\n\")\n",
        "\n",
        "            if matching:\n",
        "                # Extract and match the relation and entity\n",
        "                relation = params.get('relation', \"\").lower()  # Set default as empty string\n",
        "                entity = params.get('entity', \"\") # Set default as empty string (don't lower it)\n",
        "\n",
        "                # Match the entity to the closest in the knowledge graph. Returns an uri and label of the closest entity, and the distance\n",
        "                _, matched_entity_label, _ = match_entity_editdistance(entity, dictionary=entity_dictionary) if entity else None\n",
        "\n",
        "                # Match the relation to the closest in the knowledge graph. check_ngram_match returns a list\n",
        "                matched_predicate_label = check_ngram_match(relation, predicates, threshold=2, n=5, confidence=0.6) if relation else None\n",
        "\n",
        "                # Update entity and predicate with the matched labels\n",
        "                if matched_entity_label:\n",
        "                    params['entity'] = matched_entity_label\n",
        "                if matched_predicate_label:\n",
        "                    params['relation'] = matched_predicate_label[0]\n",
        "\n",
        "            return params\n",
        "\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c60d0cf-936f-47f6-92a9-02cbd2eb0428",
      "metadata": {
        "scrolled": true,
        "id": "7c60d0cf-936f-47f6-92a9-02cbd2eb0428",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac18ba77-58c5-4602-d768-7d54d46eee4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pattern = who is the (?P<relation>.+?) of (?P<entity>.+)\n",
            "Question matched to pattern who\n",
            "\n",
            "Pattern 0: Question: Who is the director of Star Wars\n",
            "\n",
            "relation : director\n",
            "\n",
            "entity : Star Wars\n",
            "\n",
            "type : who\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pattern = who is the (?P<relation>.+?) of (?P<entity>.+)\n",
            "pattern = (?:find|which) movies.*contain(?:s)?(?: the word)? (?P<word>\\w+)\n",
            "Question matched to pattern find_word_in_title\n",
            "\n",
            "Pattern 1: Question: Which movies whose name contains italy\n",
            "\n",
            "word : italy\n",
            "\n",
            "type : find_word_in_title\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pattern = who is the (?P<relation>.+?) of (?P<entity>.+)\n",
            "pattern = (?:find|which) movies.*contain(?:s)?(?: the word)? (?P<word>\\w+)\n",
            "pattern = (?:find|which) movies with (?P<word>\\w+) in (?:their )?titles?\n",
            "pattern = (?:find|which) movies (?:whose )?(?:title|name) contains? (?P<word>\\w+)\n",
            "pattern = (?:what are|list)(?: the)?(?: highest[-\\s]rated)? movies(?: rated)?(?: above| greater than)?(?: (?P<number>\\d+(\\.\\d+)?))?\n",
            "Question matched to pattern movies_rating_above\n",
            "\n",
            "Pattern 2: Question: List movies rated above 7\n",
            "\n",
            "number : 7\n",
            "\n",
            "type : movies_rating_above\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pattern = who is the (?P<relation>.+?) of (?P<entity>.+)\n",
            "pattern = (?:find|which) movies.*contain(?:s)?(?: the word)? (?P<word>\\w+)\n",
            "pattern = (?:find|which) movies with (?P<word>\\w+) in (?:their )?titles?\n",
            "pattern = (?:find|which) movies (?:whose )?(?:title|name) contains? (?P<word>\\w+)\n",
            "pattern = (?:what are|list)(?: the)?(?: highest[-\\s]rated)? movies(?: rated)?(?: above| greater than)?(?: (?P<number>\\d+(\\.\\d+)?))?\n",
            "pattern = movies (?:rated )?(?:above )?(?P<number>\\d+(\\.\\d+)?)?\n",
            "pattern = (?:what are|list)(?: the)?(?: lowest[-\\s]rated)? movies(?: rated)?(?: below| less than)?(?: (?P<number>\\d+(\\.\\d+)?))?\n",
            "Question matched to pattern movies_rating_below\n",
            "\n",
            "Pattern 3: Question: what are the lowest-rated movies?\n",
            "\n",
            "number : None\n",
            "\n",
            "type : movies_rating_below\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pattern = who is the (?P<relation>.+?) of (?P<entity>.+)\n",
            "pattern = (?:find|which) movies.*contain(?:s)?(?: the word)? (?P<word>\\w+)\n",
            "pattern = (?:find|which) movies with (?P<word>\\w+) in (?:their )?titles?\n",
            "pattern = (?:find|which) movies (?:whose )?(?:title|name) contains? (?P<word>\\w+)\n",
            "pattern = (?:what are|list)(?: the)?(?: highest[-\\s]rated)? movies(?: rated)?(?: above| greater than)?(?: (?P<number>\\d+(\\.\\d+)?))?\n",
            "pattern = movies (?:rated )?(?:above )?(?P<number>\\d+(\\.\\d+)?)?\n",
            "pattern = (?:what are|list)(?: the)?(?: lowest[-\\s]rated)? movies(?: rated)?(?: below| less than)?(?: (?P<number>\\d+(\\.\\d+)?))?\n",
            "pattern = movies (?:rated )?(?:below )?(?P<number>\\d+(\\.\\d+)?)?\n",
            "pattern = which (?P<entity>.+) comes first alphabetically\n",
            "Question matched to pattern entity_first_alphabetically\n",
            "\n",
            "Pattern 4: Question: Which films comes first alphabetically\n",
            "\n",
            "entity : film\n",
            "\n",
            "type : entity_first_alphabetically\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pattern = who is the (?P<relation>.+?) of (?P<entity>.+)\n",
            "pattern = (?:find|which) movies.*contain(?:s)?(?: the word)? (?P<word>\\w+)\n",
            "pattern = (?:find|which) movies with (?P<word>\\w+) in (?:their )?titles?\n",
            "pattern = (?:find|which) movies (?:whose )?(?:title|name) contains? (?P<word>\\w+)\n",
            "pattern = (?:what are|list)(?: the)?(?: highest[-\\s]rated)? movies(?: rated)?(?: above| greater than)?(?: (?P<number>\\d+(\\.\\d+)?))?\n",
            "pattern = movies (?:rated )?(?:above )?(?P<number>\\d+(\\.\\d+)?)?\n",
            "pattern = (?:what are|list)(?: the)?(?: lowest[-\\s]rated)? movies(?: rated)?(?: below| less than)?(?: (?P<number>\\d+(\\.\\d+)?))?\n",
            "pattern = movies (?:rated )?(?:below )?(?P<number>\\d+(\\.\\d+)?)?\n",
            "pattern = which (?P<entity>.+) comes first alphabetically\n",
            "pattern = list (?P<entity>.+) in alphabetical order\n",
            "pattern = which (?P<entity>.+) comes last alphabetically\n",
            "pattern = list (?P<entity>.+) in reverse alphabetical order\n",
            "Question matched to pattern entity_last_alphabetically\n",
            "\n",
            "Pattern 5: Question: list actors in reverse alphabetical order\n",
            "\n",
            "entity : actor\n",
            "\n",
            "type : entity_last_alphabetically\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "\n",
        "user_input = {\n",
        "    0: \"Who is the director of Star Wars\",\n",
        "    1: \"Which movies whose name contains italy\",\n",
        "    2: \"List movies rated above 7\",\n",
        "    3: \"what are the lowest-rated movies?\",\n",
        "    4: \"Which films comes first alphabetically\",\n",
        "    5: \"list actors in reverse alphabetical order\"\n",
        "}\n",
        "\n",
        "for pattern, question in user_input.items():\n",
        "\n",
        "    params = process_question_factual(question, nodes, predicates)\n",
        "\n",
        "    if params:\n",
        "        print(f\"Pattern {pattern}: Question: {question}\\n\")\n",
        "\n",
        "        for key, value in params.items():\n",
        "            print(f\"{key} : {params[key]}\\n\")\n",
        "        print(\"\\n\\n\")\n",
        "\n",
        "    else:\n",
        "        print(f\"\\nPattern {pattern} not matched\\n\\n\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58ab23ca-a9a9-4ff4-a254-522dd606833f",
      "metadata": {
        "id": "58ab23ca-a9a9-4ff4-a254-522dd606833f"
      },
      "source": [
        "### Step 3: Match question types to SPARQL queries\n",
        "\n",
        "The generate_sparql_query function takes the type of the question found in the params dictionary and generates the correspnding query. For example for question \"Who is the director of Star Wars\" it generates a query that query the graph looking for an entity that has relation \"director\" with the entity \"Star Wars\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f06ee64f-ec3f-4ccf-ac3a-90bed2344753",
      "metadata": {
        "id": "f06ee64f-ec3f-4ccf-ac3a-90bed2344753",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "69b1e00a-6ef2-4570-c922-44b08efe24db"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nTODO\\nyou shoud include prefixes in each query!\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "def generate_sparql_query(params):\n",
        "    qtype = params.get('type')\n",
        "\n",
        "    if qtype == 'who':\n",
        "        sparql_query = f\"\"\"\n",
        "        SELECT ?result WHERE {{\n",
        "            ?entity rdfs:label \"{params['entity']}\"@en .\n",
        "            ?entity <{pred2uri[params['relation']]}> ?item .\n",
        "            ?item rdfs:label ?result .\n",
        "            FILTER (lang(?result) = 'en')\n",
        "        }}\n",
        "        \"\"\"\n",
        "        return sparql_query\n",
        "\n",
        "\n",
        "    # Fix: this query returns names of all the entities whose label contains the word, not just movies\n",
        "    elif qtype == 'find_word_in_title':\n",
        "        word = params.get('word')\n",
        "        sparql_query = f\"\"\"\n",
        "        SELECT ?movieLabel WHERE {{\n",
        "            ?movie rdfs:label ?movieLabel .\n",
        "            FILTER(CONTAINS(LCASE(?movieLabel), LCASE(\"{word}\"))) .\n",
        "            FILTER (lang(?movieLabel) = 'en')\n",
        "        }}\n",
        "        \"\"\"\n",
        "        return sparql_query\n",
        "\n",
        "    elif qtype == 'movies_rating_above':\n",
        "        number = params.get('number')\n",
        "        sparql_query = f\"\"\"\n",
        "        SELECT ?movieLabel WHERE {{\n",
        "            ?movie ddis:rating ?rating .\n",
        "            FILTER(?rating > {number}) .\n",
        "            ?movie rdfs:label ?movieLabel .\n",
        "            FILTER (lang(?movieLabel) = 'en')\n",
        "        }} ORDER BY DESC(?rating) LIMIT 1\n",
        "        \"\"\"\n",
        "        return sparql_query\n",
        "\n",
        "    elif qtype == 'movies_rating_below':\n",
        "        number = params.get('number')\n",
        "        sparql_query = f\"\"\"\n",
        "        SELECT ?movieLabel WHERE {{\n",
        "            ?movie ddis:rating ?rating .\n",
        "            FILTER(?rating < {number}) .\n",
        "            ?movie rdfs:label ?movieLabel .\n",
        "            FILTER (lang(?movieLabel) = 'en')\n",
        "        }} ORDER BY DESC(?rating)\n",
        "        \"\"\"\n",
        "        return sparql_query\n",
        "\n",
        "    elif qtype == 'entity_first_alphabetically':\n",
        "        sparql_query = f\"\"\"\n",
        "        SELECT ?entity_label WHERE {{\n",
        "            ?entity wdt:P31 <{params['matched_entity_uri']}> .\n",
        "            ?entity rdfs:label ?entity_label .\n",
        "            FILTER (lang(?entity_label) = 'en')\n",
        "        }} ORDER BY ASC(?entity_label)\n",
        "        \"\"\"\n",
        "        return sparql_query\n",
        "\n",
        "    elif qtype == 'entity_last_alphabetically':\n",
        "        sparql_query = f\"\"\"\n",
        "        SELECT ?entity_label WHERE {{\n",
        "            ?entity wdt:P31 <{params['matched_entity_uri']}> .\n",
        "            ?entity rdfs:label ?entity_label .\n",
        "            FILTER (lang(?entity_label) = 'en')\n",
        "        }} ORDER BY DESC(?entity_label)\n",
        "        \"\"\"\n",
        "        return sparql_query\n",
        "\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "'''\n",
        "TODO\n",
        "you shoud include prefixes in each query!\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3470a349-2778-497f-baf4-c42997a1075c",
      "metadata": {
        "id": "3470a349-2778-497f-baf4-c42997a1075c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "141dc4b9-f0cd-4b23-a4bc-f594613f0f08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Star Wars']\n"
          ]
        }
      ],
      "source": [
        "print(check_ngram_match('Star Wars', nodes))\n",
        "\n",
        "sparql_query = f\"\"\"\n",
        "        SELECT ?result WHERE {{\n",
        "            ?entity rdfs:label \"{check_ngram_match('Star Wars', nodes)[0]}\"@en .\n",
        "            ?entity <{pred2uri['director']}> ?item .\n",
        "            ?item rdfs:label ?result .\n",
        "            FILTER (lang(?result) = 'en')\n",
        "        }}\n",
        "        \"\"\"\n",
        "for res in graph.query(sparql_query):\n",
        "    print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4aaab09c-835c-48bc-bb72-ea369dd49867",
      "metadata": {
        "id": "4aaab09c-835c-48bc-bb72-ea369dd49867"
      },
      "source": [
        "### Step 4: Query the graph\n",
        "\n",
        "We implement a function that takes the graph and query and returns the result and an exit code that is an empty string if results were found, and \"No results\" if no results were found"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8257008-8701-4d28-be80-7a219e00fcc0",
      "metadata": {
        "id": "f8257008-8701-4d28-be80-7a219e00fcc0"
      },
      "outputs": [],
      "source": [
        "def query_graph(graph, sparql_query):\n",
        "\n",
        "    # Execute the query\n",
        "    qres = graph.query(sparql_query)\n",
        "\n",
        "    # Process the results\n",
        "    results = []\n",
        "    for row in qres:\n",
        "        results.append(str(row.result))\n",
        "\n",
        "    # Check if we have results, if not return exit_code = \"No results\"\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "143e8988-f5cc-4f00-92a0-2e37bf67821d",
      "metadata": {
        "id": "143e8988-f5cc-4f00-92a0-2e37bf67821d"
      },
      "source": [
        "### Step 5: Put everything together\n",
        "\n",
        "We implement a function that takes a question and computes the factual answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acf11be8-3cef-488e-a917-6d065100b4f7",
      "metadata": {
        "id": "acf11be8-3cef-488e-a917-6d065100b4f7"
      },
      "outputs": [],
      "source": [
        "def answer_question_factual(question):\n",
        "\n",
        "    if process_question_factual(question, nodes, predicates):\n",
        "        params = process_question_factual(question, nodes, predicates)\n",
        "        print(f\"Parameters found: {params}\\n\")\n",
        "    else:\n",
        "        return exit_message\n",
        "\n",
        "    sparql_query = generate_sparql_query(params)\n",
        "\n",
        "    answer = query_graph(graph, sparql_query)\n",
        "\n",
        "    if answer:\n",
        "        return answer\n",
        "    else:\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a74e19b-a2f0-4d2e-9e67-b138468a308e",
      "metadata": {
        "id": "9a74e19b-a2f0-4d2e-9e67-b138468a308e"
      },
      "outputs": [],
      "source": [
        "def answer_question_factual(question):\n",
        "\n",
        "    try:\n",
        "\n",
        "        # Process question to get parameters\n",
        "        params = process_question_factual(question, nodes, predicates)\n",
        "\n",
        "        print(f\"Parameters found: {params}\\n\")\n",
        "\n",
        "        # Generate the SPARQL query based on the extracted parameters\n",
        "        sparql_query = generate_sparql_query(params)\n",
        "\n",
        "        # Execute the query on the knowledge graph\n",
        "        answer = query_graph(graph, sparql_query)\n",
        "\n",
        "        # Check if an answer was returned\n",
        "        if answer:\n",
        "            return answer\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "\n",
        "        return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf1273fa-ea5c-4bee-abd5-623fdb2a538f",
      "metadata": {
        "id": "cf1273fa-ea5c-4bee-abd5-623fdb2a538f"
      },
      "source": [
        "# Final Chatbot Structure\n",
        "\n",
        "We have successfully implemented a way to retrieve embeddings answers and factual answers. Now we put everything together and implement a way for the chatbot to seem as human as possible"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "daaf6ed8-f10f-4786-9386-460bad67c514",
      "metadata": {
        "id": "daaf6ed8-f10f-4786-9386-460bad67c514"
      },
      "outputs": [],
      "source": [
        "def generate_answer(graph, question):\n",
        "\n",
        "    #questions_list = split_questions(question)\n",
        "    questions_list = [question]\n",
        "\n",
        "\n",
        "    Answer = \"\"\n",
        "\n",
        "    if len(questions_list) > 1:\n",
        "\n",
        "        for i, q in enumerate(questions_list):\n",
        "\n",
        "            embedding_answer = answer_question_embeddings(q)\n",
        "\n",
        "            print(\"\\n\\n-----------\\n\\n\")\n",
        "\n",
        "            factual_answer = answer_question_factual(q)\n",
        "\n",
        "            print(\"\\n\\n-----------\\n\\n\")\n",
        "\n",
        "            Answer += f\"For question {q} the answer suggested by the embeddings is: {embedding_answer} while the answer obtained by quering the graph is: {factual_answer}\\n\"\n",
        "\n",
        "    else:\n",
        "\n",
        "        if answer_question_embeddings(question):\n",
        "\n",
        "            embedding_answer = answer_question_embeddings(question)\n",
        "\n",
        "            Answer += f\"The answer suggested by the embeddings is: {embedding_answer}\\n\"\n",
        "\n",
        "        print(\"\\n\\n-----------\\n\\n\")\n",
        "\n",
        "        if answer_question_factual(question):\n",
        "\n",
        "            factual_answer = answer_question_factual(question)\n",
        "\n",
        "            Answer += f\"The answer obtained by querying the graph is: {factual_answer}\\n\"\n",
        "\n",
        "        else:\n",
        "            # Generate a response message with pegasus\n",
        "            num_beams = 10\n",
        "            num_return_sequences = 1\n",
        "            context = f\"{question} i don't know\"\n",
        "            print(context)\n",
        "            print(get_response(context,num_return_sequences,num_beams)[0])\n",
        "            print(\"\\n\")\n",
        "            Answer += get_response(context,num_return_sequences,num_beams)[0]\n",
        "\n",
        "\n",
        "        print(\"\\n\\n-----------\\n\\n\")\n",
        "\n",
        "    return Answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ab394d7-f925-48a9-bf09-a9aee3f63e97",
      "metadata": {
        "id": "1ab394d7-f925-48a9-bf09-a9aee3f63e97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 880
        },
        "outputId": "167f2e15-e62d-41a4-b3ad-5b31168d391a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question after preprocessing: Who is the director of Star Wars and who is the screenwriter of The Godfather\n",
            "\n",
            "Extracted entity: Star Wars\n",
            "\n",
            "Extracted entity: The Godfather\n",
            "\n",
            "Question after removing entities: who is the director of and who is the screenwriter of\n",
            "\n",
            "Question after preprocessing: Who is the director of Star Wars and who is the screenwriter of The Godfather\n",
            "\n",
            "Extracted entity: Star Wars\n",
            "\n",
            "Extracted entity: The Godfather\n",
            "\n",
            "Question after removing entities: who is the director of and who is the screenwriter of\n",
            "\n",
            "\n",
            "\n",
            "-----------\n",
            "\n",
            "\n",
            "pattern = who is the (?P<relation>.+?) of (?P<entity>.+)\n",
            "Question matched to pattern who\n",
            "\n",
            "Who is the director of Star Wars and who is the screenwriter of The Godfather i don't know\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "The current model class (BertForTokenClassification) is not compatible with `.generate()`, as it doesn't have a language model head. Classes that support generation often end in one of these names: ['ForCausalLM', 'ForConditionalGeneration', 'ForSpeechSeq2Seq', 'ForVision2Seq'].",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-2ea775dcf2c9>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Who is the director of Star Wars and who is the screenwriter of The Godfather\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-75-e5f07543a0ef>\u001b[0m in \u001b[0;36mgenerate_answer\u001b[0;34m(graph, question)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{question} i don't know\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_return_sequences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_beams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mAnswer\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mget_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_return_sequences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_beams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-388b02906732>\u001b[0m in \u001b[0;36mget_response\u001b[0;34m(input_text, num_return_sequences, num_beams)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_return_sequences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_beams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'longest'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mtranslated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_beams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_beams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_return_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_return_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0mtgt_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtgt_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1966\u001b[0m         \u001b[0;31m# 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1967\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1968\u001b[0m         \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tokenizer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Pull this out first, we only use it for stopping criteria\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1969\u001b[0m         \u001b[0massistant_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"assistant_tokenizer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# only used for assisted generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_validate_model_class\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1267\u001b[0m                 \u001b[0;34m\"ForVision2Seq\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m             ]\n\u001b[0;32m-> 1269\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m   1270\u001b[0m                 \u001b[0;34mf\"The current model class ({self.__class__.__name__}) is not compatible with `.generate()`, as \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m                 \u001b[0;34m\"it doesn't have a language model head. Classes that support generation often end in one of these \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: The current model class (BertForTokenClassification) is not compatible with `.generate()`, as it doesn't have a language model head. Classes that support generation often end in one of these names: ['ForCausalLM', 'ForConditionalGeneration', 'ForSpeechSeq2Seq', 'ForVision2Seq']."
          ]
        }
      ],
      "source": [
        "# Test\n",
        "\n",
        "question = \"Who is the director of Star Wars and who is the screenwriter of The Godfather\"\n",
        "\n",
        "print(generate_answer(graph, question))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question6 = \"Given that I like The Lion King, Pocahontas, and The Beauty and the Beast, can you recommend some movies?\"\n",
        "\n",
        "#print(generate_answer(graph, question1))\n",
        "print(generate_answer(graph, question6))\n"
      ],
      "metadata": {
        "id": "lwkYGSZv1V2_"
      },
      "id": "lwkYGSZv1V2_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bobbnk.mpo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "J6Jd076O-e10",
        "outputId": "f609279e-3d15-4e95-b9a5-ef4919aaab42"
      },
      "id": "J6Jd076O-e10",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'bobbnk' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-b95cf06036bb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbobbnk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmpo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'bobbnk' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb16d316-57c4-4c9d-99ec-b8d41738bfa3",
      "metadata": {
        "id": "fb16d316-57c4-4c9d-99ec-b8d41738bfa3"
      },
      "source": [
        "# SpeakEasy Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a31630b9-a284-4944-a693-ad3e8cc549d3",
      "metadata": {
        "id": "a31630b9-a284-4944-a693-ad3e8cc549d3"
      },
      "outputs": [],
      "source": [
        "from rdflib.namespace import Namespace, RDF, RDFS, XSD\n",
        "from rdflib.term import URIRef, Literal\n",
        "import rdflib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install '/content/drive/MyDrive/Speakeasy_Project/speakeasy-python-client-library/dist/speakeasypy-1.0.0-py3-none-any.whl'\n",
        "\n",
        "from speakeasypy import Speakeasy, Chatroom\n",
        "from typing import List\n",
        "import time\n",
        "\n",
        "DEFAULT_HOST_URL = 'https://speakeasy.ifi.uzh.ch'\n",
        "listen_freq = 2\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, username, password):\n",
        "        self.username = username\n",
        "        # Initialize the Speakeasy Python framework and login.\n",
        "        self.speakeasy = Speakeasy(host=DEFAULT_HOST_URL, username=username, password=password)\n",
        "        self.speakeasy.login()  # This framework will help you log out automatically when the program terminates.\n",
        "\n",
        "    def listen(self):\n",
        "        graph = rdflib.Graph()\n",
        "        #graph.parse('/Users/gianmarcoalbano/Desktop/Advanced topics in AI/Speakeasy Project/Datasets/14_graph.nt', format='turtle')\n",
        "        graph.parse('/content/drive/MyDrive/14_graph.nt', format='turtle')\n",
        "        while True:\n",
        "            # only check active chatrooms (i.e., remaining_time > 0) if active=True.\n",
        "            rooms: List[Chatroom] = self.speakeasy.get_rooms(active=True)\n",
        "            for room in rooms:\n",
        "                if not room.initiated:\n",
        "                    # send a welcome message if room is not initiated\n",
        "                    room.post_messages(f'Hello! This is a welcome message from {room.my_alias}.')\n",
        "                    room.initiated = True\n",
        "                # Retrieve messages from this chat room.\n",
        "                # If only_partner=True, it filters out messages sent by the current bot.\n",
        "                # If only_new=True, it filters out messages that have already been marked as processed.\n",
        "                for message in room.get_messages(only_partner=True, only_new=True):\n",
        "                    print(\n",
        "                        f\"\\t- Chatroom {room.room_id} \"\n",
        "                        f\"- new message #{message.ordinal}: '{message.message}' \"\n",
        "                        f\"- {self.get_time()}\")\n",
        "\n",
        "                    # Implement your agent here #\n",
        "                    result = generate_answer(graph, message.message)\n",
        "\n",
        "                    # Send a message to the corresponding chat room using the post_messages method of the room object.\n",
        "                    room.post_messages(f\"Received your message: '{result}' \")\n",
        "                    # Mark the message as processed, so it will be filtered out when retrieving new messages.\n",
        "                    room.mark_as_processed(message)\n",
        "\n",
        "                # Retrieve reactions from this chat room.\n",
        "                # If only_new=True, it filters out reactions that have already been marked as processed.\n",
        "                for reaction in room.get_reactions(only_new=True):\n",
        "                    print(\n",
        "                        f\"\\t- Chatroom {room.room_id} \"\n",
        "                        f\"- new reaction #{reaction.message_ordinal}: '{reaction.type}' \"\n",
        "                        f\"- {self.get_time()}\")\n",
        "\n",
        "                    # Implement your agent here #\n",
        "\n",
        "                    room.post_messages(f\"Received your reaction: '{reaction.type}' \")\n",
        "                    room.mark_as_processed(reaction)\n",
        "\n",
        "            time.sleep(listen_freq)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_time():\n",
        "        return time.strftime(\"%H:%M:%S, %d-%m-%Y\", time.localtime())\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    demo_bot = Agent(\"swift-comet\", \"X2wqU6D3\")\n",
        "    demo_bot.listen()"
      ],
      "metadata": {
        "id": "XzcNaTfqsPcg"
      },
      "id": "XzcNaTfqsPcg",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}