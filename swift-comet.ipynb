{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "510c39a3-dfef-4dd7-b2c4-893d62dbf561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "from rdflib import Graph\n",
    "from transformers import pipeline\n",
    "from fuzzywuzzy import process\n",
    "from rdflib.term import URIRef, Literal\n",
    "import torch\n",
    "import numpy as np\n",
    "import editdistance\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import re\n",
    "import random\n",
    "from rdflib.namespace import Namespace, RDF, RDFS, XSD\n",
    "import pandas as pd\n",
    "from statsmodels.stats.inter_rater import fleiss_kappa\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75b0850-8f01-47d0-a807-16a7c9a14581",
   "metadata": {},
   "source": [
    "## Import the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c599047c-b6be-4bee-8407-20b81a27b410",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/zmq/backend/cython/checkrc.pxd:13\u001b[0m, in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'zmq.backend.cython.message.Frame.__dealloc__'\n",
      "Traceback (most recent call last):\n",
      "  File \"zmq/backend/cython/checkrc.pxd\", line 13, in zmq.backend.cython.checkrc._check_rc\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Graph identifier=Nfc6ac4a843944b75b2df3a175faf0d56 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and load a sample knowledge graph\n",
    "graph = rdflib.Graph()\n",
    "graph.parse('14_graph.nt', format='turtle') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8474a48-1773-407e-8576-71f4893504d9",
   "metadata": {},
   "source": [
    "## Load Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c027c69-4b03-42b5-b664-ee521ecff070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Frameworks/Python.framework/Versions/3.12/bin/python3.12 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# this command downloads the Spacy model\n",
    "spacy.cli.download(\"en_core_web_md\")\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caed353e-0bbd-4d5c-b9f1-f627403955e2",
   "metadata": {},
   "source": [
    "## Load the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a52c307d-5c9c-4ff8-924e-293cdd6d28b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "entity_matrix = np.load('/Users/gianmarcoalbano/Desktop/Advanced topics in AI/Chatbot-Project/ddis-graph-embeddings/entity_embeds.npy')\n",
    "predicate_matrix = np.load('/Users/gianmarcoalbano/Desktop/Advanced topics in AI/Chatbot-Project/ddis-graph-embeddings/relation_embeds.npy')\n",
    "\n",
    "with open('/Users/gianmarcoalbano/Desktop/Advanced topics in AI/Chatbot-Project/ddis-graph-embeddings/entity_ids.del') as ifile:\n",
    "    ent2id = {ent: int(idx) for idx, ent in csv.reader(ifile, delimiter='\\t')}\n",
    "    id2ent = {v: k for k, v in ent2id.items()}\n",
    "with open('/Users/gianmarcoalbano/Desktop/Advanced topics in AI/Chatbot-Project/ddis-graph-embeddings/relation_ids.del') as ifile:\n",
    "    pred2id = {rel: int(idx) for idx, rel in csv.reader(ifile, delimiter='\\t')}\n",
    "    id2pred = {v: k for k, v in pred2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f614461-5489-46da-a5f1-4654c4d6b7ba",
   "metadata": {},
   "source": [
    "## Build dictionaries for entities and predicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69f32caf-313d-4ed8-a7eb-88926472df0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "namespaces = {\n",
    "    'WD': rdflib.Namespace(\"http://www.wikidata.org/entity/\"),\n",
    "    'WDT': rdflib.Namespace(\"http://www.wikidata.org/prop/direct/\"),\n",
    "    'DDIS': rdflib.Namespace(\"http://ddis.ch/atai/\"),\n",
    "    'RDFS': rdflib.namespace.RDFS,\n",
    "    'SCHEMA': rdflib.Namespace(\"http://schema.org/\")\n",
    "}\n",
    "\n",
    "nodes = {\n",
    "    node.toPython(): str(graph.value(node, namespaces['RDFS'].label) \n",
    "                         or str(node).split('/')[-1])\n",
    "    for node in graph.all_nodes() if isinstance(node, rdflib.term.URIRef)\n",
    "}\n",
    "ent2uri = {ent: uri for uri, ent in nodes.items()}\n",
    "\n",
    "predicates = {\n",
    "    node.toPython(): str(graph.value(node, namespaces['RDFS'].label) \n",
    "                         or str(node).split('/')[-1])\n",
    "    for node in graph.predicates() if isinstance(node, rdflib.term.URIRef)\n",
    "}\n",
    "pred2uri = {pred: uri for uri, pred in predicates.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d7da5c-9ee3-4e5b-99ab-b03c57609894",
   "metadata": {},
   "source": [
    "## Build people-occupation dictionary using SPARQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45cccd10-e782-4265-8f8e-27934681514f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "SELECT DISTINCT ?person_label ?occupation_label WHERE {{\n",
    "    ?person <{namespaces['WDT'].P106}> ?occupation .\n",
    "    ?person <{namespaces['RDFS'].label}> ?person_label .\n",
    "    ?occupation <{namespaces['RDFS'].label}> ?occupation_label .\n",
    "}}\n",
    "\"\"\"\n",
    "result = graph.query(query)\n",
    "\n",
    "# Create dictionary mapping people to occupations\n",
    "people_occupation = {}\n",
    "for row in result:\n",
    "    person = str(row.person_label)\n",
    "    occupation = str(row.occupation_label)\n",
    "\n",
    "    if person not in people_occupation:\n",
    "        people_occupation[person] = []\n",
    "    if occupation not in people_occupation[person]:\n",
    "        people_occupation[person].append(occupation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088a7e1b-34b0-40ca-9729-617823334a9b",
   "metadata": {},
   "source": [
    "## Build a dictionary mapping people to movies they contributed to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09b3b3bc-46af-4d28-ab14-dab6f2f2265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_movies = {}\n",
    "\n",
    "query = \"\"\"\n",
    "PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "\n",
    "SELECT ?personLabel ?movieLabel WHERE {\n",
    "    ?movie wdt:P161|wdt:P57|wdt:P58|wdt:P344|wdt:P3300|wdt:P1809 ?person .\n",
    "    ?person rdfs:label ?personLabel .\n",
    "    ?movie rdfs:label ?movieLabel .\n",
    "    FILTER (lang(?personLabel) = 'en' && lang(?movieLabel) = 'en')\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    qres = graph.query(query)\n",
    "\n",
    "    for row in qres:\n",
    "        person = str(row.personLabel)\n",
    "        movie = str(row.movieLabel)\n",
    "        if person not in people_movies:\n",
    "            people_movies[person] = []\n",
    "        people_movies[person].append(movie)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error building people-to-movies dictionary: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ad99c67-1d38-49bd-920c-fe218cc2e18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass dictionaries to the chatbot class\n",
    "dictionaries = (nodes, ent2uri, predicates, pred2uri, people_occupation, ent2id, id2ent, pred2id, id2pred, entity_matrix, predicate_matrix, people_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584dce3a-0672-4ed2-aeb8-e69698d702b7",
   "metadata": {},
   "source": [
    "# Load the image file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77f13efc-47b5-48ff-b489-8a40a24f70e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"images.json\", \"r\") as f:\n",
    "    images = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7bf451-ac1a-4e35-bcb7-f86c2681a970",
   "metadata": {},
   "source": [
    "## Handle the crowdsourcing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a35561f8-0c10-4532-857c-9c1b7673f4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Crowdsourced Data\n",
    "def preprocess_crowd_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses the crowdsourced data.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(file_path, sep='\\t')\n",
    "    data['LifetimeApprovalRate'] = data['LifetimeApprovalRate'].replace('%', '', regex=True).astype(float)\n",
    "    data['WorkTimeInSeconds'] = pd.to_numeric(data['WorkTimeInSeconds'], errors='coerce')\n",
    "    return data\n",
    "\n",
    "\n",
    "# Filter Workers\n",
    "def filter_workers(data, approval_threshold=50, time_threshold=35):\n",
    "    \"\"\"\n",
    "    Filters workers based on thresholds.\n",
    "    Returns filtered DataFrame of valid workers.\n",
    "    \"\"\"\n",
    "    valid_workers = data[\n",
    "        (data['LifetimeApprovalRate'] >= approval_threshold) & \n",
    "        (data['WorkTimeInSeconds'] >= time_threshold)\n",
    "    ]\n",
    "    valid_workers = valid_workers.sort_values(by='WorkTimeInSeconds', ascending=False)\n",
    "    valid_workers = valid_workers.groupby('HITId').head(3)\n",
    "    return valid_workers\n",
    "\n",
    "def calculate_fleiss_kappa_per_batch(valid_workers):\n",
    "    \"\"\"\n",
    "    Calculates Fleiss' Kappa for each HITTypeId batch.\n",
    "    Returns a dictionary mapping HITTypeId to Fleiss' Kappa value.\n",
    "    \"\"\"\n",
    "    contingency_tables = valid_workers.groupby(['HITTypeId', 'HITId'])['AnswerLabel'].value_counts().unstack(fill_value=0)\n",
    "    fleiss_kappa_per_batch = contingency_tables.groupby(level=0).apply(\n",
    "        lambda x: fleiss_kappa(x.to_numpy()) if not x.empty else -1\n",
    "    ).to_dict()\n",
    "\n",
    "    return fleiss_kappa_per_batch\n",
    "\n",
    "\n",
    "file_path = '/Users/gianmarcoalbano/Desktop/Advanced topics in AI/Chatbot-Project/crowd_data.tsv'\n",
    "data = preprocess_crowd_data(file_path)\n",
    "valid_workers = filter_workers(data)\n",
    "fleiss_kappa_per_batch = calculate_fleiss_kappa_per_batch(valid_workers)\n",
    "\n",
    "crowdsourcing_files = (data, valid_workers, fleiss_kappa_per_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a7822f37-7e40-4a63-a12d-c8ab1f83faf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chatbot:\n",
    "    def __init__(self, graph, dictionaries, images, crowsourcing_files):\n",
    "\n",
    "        self.graph = graph\n",
    "        \n",
    "        # Initialize zero-shot classification pipeline\n",
    "        self.classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "        # Load the NER model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
    "        model = AutoModelForTokenClassification.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
    "        self.ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, grouped_entities=True)\n",
    "\n",
    "        # Define namespaces\n",
    "        self.namespaces = {\n",
    "            'WD': rdflib.Namespace(\"http://www.wikidata.org/entity/\"),\n",
    "            'WDT': rdflib.Namespace(\"http://www.wikidata.org/prop/direct/\"),\n",
    "            'DDIS': rdflib.Namespace(\"http://ddis.ch/atai/\"),\n",
    "            'RDFS': rdflib.namespace.RDFS,\n",
    "            'SCHEMA': rdflib.Namespace(\"http://schema.org/\")\n",
    "        }\n",
    "\n",
    "        self.nodes, self.ent2uri, self.predicates, self.pred2uri, self.people_occupation, self.ent2id, self.id2ent, self.pred2id, self.id2pred, self.entity_matrix, self.predicate_matrix, self.people_movies = dictionaries\n",
    "\n",
    "        self.images = images\n",
    "\n",
    "        self.expecting_response = False\n",
    "        self.last_question_info = {\"question\": None, \"suggested_label\": None, \"updated_label\": None}\n",
    "\n",
    "        self.crowd_data, self.valid_workers, self.fleiss_kappa_per_batch = crowsourcing_files\n",
    "\n",
    "        # Initialize Pegasus model for paraphrasing\n",
    "        model_name = 'tuner007/pegasus_paraphrase'\n",
    "        self.tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "        self.paraphrase_model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "        # --- Factual Question Patterns ---\n",
    "        self.question_patterns = [\n",
    "        \n",
    "            # Pattern 0: who and what\n",
    "            (r\"who is the (?P<relation>.+?) of (?P<entity>.+)\", 'who', 1),\n",
    "            (r\"who (?P<relation>.+?) (?P<entity>.+)\", 'who', 1),\n",
    "        \n",
    "            # Pattern 1: Find movies with (word) in their titles\n",
    "            (r\"(?:find|which) movies.*contain(?:s)?(?: the word)? (?P<word>\\w+)\", 'find_word_in_title', 0),\n",
    "            (r\"(?:find|which) movies with (?P<word>\\w+) in (?:their )?titles?\", 'find_word_in_title', 0),\n",
    "            (r\"(?:find|which) movies (?:whose )?(?:title|name) contains? (?P<word>\\w+)\", 'find_word_in_title', 0),\n",
    "        \n",
    "            # Pattern 2: Highest-rated movies (optional 'above' and a number)\n",
    "            (r\"(?:what are|list)(?: the)?(?: highest[-\\s]rated)? movies(?: rated)?(?: above| greater than)?(?: (?P<number>\\d+(\\.\\d+)?))?\", 'movies_rating_above', 0),\n",
    "            (r\"movies (?:rated )?(?:above )?(?P<number>\\d+(\\.\\d+)?)?\", 'movies_rating_above', 0),\n",
    "        \n",
    "            # Pattern 3: Lowest-rated movies (optional 'below' and a number)\n",
    "            (r\"(?:what are|list)(?: the)?(?: lowest[-\\s]rated)? movies(?: rated)?(?: below| less than)?(?: (?P<number>\\d+(\\.\\d+)?))?\", 'movies_rating_below', 0),\n",
    "            (r\"movies (?:rated )?(?:below )?(?P<number>\\d+(\\.\\d+)?)?\", 'movies_rating_below', 0),\n",
    "        \n",
    "            # Pattern 4: Entities in alphabetical order\n",
    "            (r\"which (?P<entity>.+) comes first alphabetically\", 'entity_first_alphabetically', 1),\n",
    "            (r\"list (?P<entity>.+) in alphabetical order\", 'entity_first_alphabetically', 1),\n",
    "        \n",
    "            # Pattern 5: Entities in reverse alphabetical order\n",
    "            (r\"which (?P<entity>.+) comes last alphabetically\", 'entity_last_alphabetically', 1),\n",
    "            (r\"list (?P<entity>.+) in reverse alphabetical order\", 'entity_last_alphabetically', 1),\n",
    "\n",
    "            # Pattern 6: Release date of an entity\n",
    "            (r\"when was (?P<entity>.+?) released\\??\", 'release_date', 1),\n",
    "            (r\"what is the release date of (?P<entity>.+?)\\??\", 'release_date', 1),\n",
    "            (r\"when did (?P<entity>.+?) come out\\??\", 'release_date', 1),\n",
    "            (r\"when did (?P<entity>.+?) (?:premiere|debut)\\??\", 'release_date', 1),\n",
    "            (r\"(?:pubblication|release) date of (?P<entity>.+?)\\??\", 'release_date', 1),\n",
    "        \n",
    "        ]\n",
    "\n",
    "    ##############################################################################\n",
    "    ##############################################################################\n",
    "    #######################                                #######################\n",
    "    #######################              ANSWER            #######################\n",
    "    #######################              ------            #######################\n",
    "    #######################                                #######################\n",
    "    ##############################################################################\n",
    "    ##############################################################################\n",
    "\n",
    "    def answer_question(self, question):\n",
    "\n",
    "        try:\n",
    "            main_labels = [\"image request\", \"recommendation query\", \"movie query\", \"general query\"]\n",
    "\n",
    "            if self.expecting_response == False:\n",
    "                question_type = self.identify_question(question, main_labels, uncertainty=True)\n",
    "                if self.expecting_response:\n",
    "                    return f\"I’m not entirely sure, but are you asking about {self.last_question_info[\"suggested_label\"][0]} or {self.last_question_info[\"suggested_label\"][1]}?\"\n",
    "\n",
    "            else:\n",
    "                possible_labels = [self.last_question_info[\"suggested_label\"][0], self.last_question_info[\"suggested_label\"][1], \"negative statement\"]\n",
    "                question_type = self.identify_question(question, possible_labels)\n",
    "                if question_type != \"negative statement\":\n",
    "                    question = self.last_question_info[\"question\"]\n",
    "                    self.reset_last_question_info()\n",
    "                else:\n",
    "                    self.reset_last_question_info()\n",
    "                    return f\"I am still not sure about the question, could you rephrase it?\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "            return \"Sorry, something went wrong while processing your question.\"\n",
    "\n",
    "        \n",
    "        # Handle Image Requests\n",
    "        if question_type == \"image request\":\n",
    "            entity_name, _ = self.extract_known_entity(question, self.people_occupation)\n",
    "            entities = self.NER(question, self.people_occupation)\n",
    "            if len(entities) == 1 and entities[0]['entity_group'] == 'PER':\n",
    "                entity_name = entities[0]['word']\n",
    "            if not entity_name:\n",
    "                return \"I couldn’t find any known person in your question.\"\n",
    "            imdb_id = self.get_imdb_id(entity_name)\n",
    "            if not imdb_id:\n",
    "                return f\"Sorry, I couldn’t find an IMDb ID for {entity_name}.\"\n",
    "            image_path = self.get_image_from_json(imdb_id)\n",
    "            if not image_path:\n",
    "                return f\"Sorry, I couldn’t find any image for {entity_name}.\"\n",
    "            return f\"Found Image for {entity_name}: {image_path}\"\n",
    "                \n",
    "        # Handle Movie Queries\n",
    "        if question_type == \"movie query\":\n",
    "\n",
    "            # Try CROWDSOURCING\n",
    "            \n",
    "            # Extract predicates and entities for crowdsourcing and Embeddings\n",
    "            extracted_predicate, entities = self.extract_entities_and_predicates(question, self.ent2uri)\n",
    "\n",
    "            if not extracted_predicate or not entities:\n",
    "                return \"Sorry, my head is all over the place, could you rephrase your question?\"\n",
    "\n",
    "            if len(entities) > 1:\n",
    "                self.expecting_response = True\n",
    "                return f\"I’m not entirely sure, but are you asking about {self.last_question_info[\"suggested_label\"][0]} or {self.last_question_info[\"suggested_label\"][1]}? Sorry but i get confused if i need to handle multiple stuff at once hahaha\"\n",
    "\n",
    "            entity = entities[0]['word']\n",
    "            \n",
    "            # Find entity and predicate in crowdsourced data\n",
    "            crowdsourcing_entity = self.find_entity_in_crowd_data(entity, 'entity')\n",
    "            crowdsourcing_predicate = self.find_entity_in_crowd_data(extracted_predicate, 'predicate')\n",
    "    \n",
    "            if crowdsourcing_entity is not None and crowdsourcing_predicate is not None:\n",
    "                answer = self.crowdsource_search_with_valid_workers(crowdsourcing_entity, crowdsourcing_predicate)\n",
    "                if answer:\n",
    "                    return answer\n",
    "\n",
    "            # Try FACTUAL ANSWER\n",
    "            params = self.match_question_pattern(question)\n",
    "            if params:\n",
    "                # Extract additional data only if required\n",
    "                if params['requires_matching']:\n",
    "                    extracted_predicate, entities = self.extract_entities_and_predicates(question, self.ent2uri)\n",
    "                    if extracted_predicate:\n",
    "                        params['relation'] = extracted_predicate\n",
    "                    if entities:\n",
    "                        matched_entity = entities[0]['word']\n",
    "                        params['entity'] = matched_entity\n",
    "\n",
    "                sparql_query = self.generate_sparql_query(params)\n",
    "                if sparql_query:\n",
    "                    answer = self.query_graph(sparql_query)\n",
    "                    if answer:\n",
    "                        response = self.paraphrase(\"I am absolutely sure that the answer is\") + \" \" + ', '.join(answer)\n",
    "                        return response\n",
    "\n",
    "            # Refine Question Type if Factual Query Fails \n",
    "            refined_labels = [\"release date or publication date query\", \"movie query\"]\n",
    "            refined_type = self.identify_question(question, refined_labels)\n",
    "\n",
    "            if refined_type == \"release date or publication date query\":\n",
    "                print(\"Identified Release Date question\")\n",
    "                params = {\n",
    "                    \"type\": \"release_date\",\n",
    "                    \"entity\": self.NER(question, self.ent2uri)[0]['word'],\n",
    "                }\n",
    "                sparql_query = self.generate_sparql_query(params)\n",
    "                answer = self.query_graph(sparql_query)\n",
    "                return f\"The release Date of {matched_entity} is {', '.join(answer)}\" if answer else \"Sorry, I couldn’t find the release date.\"\n",
    "                 \n",
    "                \n",
    "            # Fallback to Embeddings for Remaining Movie Queries\n",
    "            answer = self.compute_embedding_answer(extracted_predicate, entity)\n",
    "            \n",
    "            if not answer:\n",
    "                return \"Sorry, but it's been a hard day and I really don't feel like answering this question :(\"\n",
    "            \n",
    "            # Format the answer list nicely\n",
    "            if len(answer) == 1:\n",
    "                answer_str = answer[0]\n",
    "            elif len(answer) == 2:\n",
    "                answer_str = ' and '.join(answer)\n",
    "            else:\n",
    "                answer_str = ', '.join(answer[:-1]) + f\" or maybe {answer[-1]}\"\n",
    "            \n",
    "            return f\"I think that the {extracted_predicate} of {entity} might be {answer_str}. Not sure which one tbh\"\n",
    "\n",
    "    \n",
    "        # Handle Recommendation Queries\n",
    "        if question_type == \"recommendation query\":\n",
    "            try:\n",
    "                # Extract movies using NER\n",
    "                entities = self.NER(question, self.ent2uri)\n",
    "                if not entities:\n",
    "                    return \"I would need to know a bit more about your preferences to give you a good suggestion, for now I can tell you that my favourite movie in the world is Les Miserables!\"\n",
    "                liked_people = [ent['word'] for ent in entities if ent['entity_group'] == 'PER']\n",
    "                liked_movies = [ent['word'] for ent in entities if ent['entity_group'] != 'PER']\n",
    "\n",
    "                # Add movies associated with liked people\n",
    "                if liked_people:\n",
    "                    for person in liked_people:\n",
    "                        movies = self.people_movies.get(person, [])[:3]\n",
    "                        liked_movies.extend(movies)\n",
    "        \n",
    "                # Compute recommendations\n",
    "                recommendations = self.compute_recommendation(liked_movies)\n",
    "                if not recommendations:\n",
    "                    return \"Sorry, I couldn’t find any recommendations based on your preferences.\"\n",
    "        \n",
    "                return f\"Recommended Movies: {', '.join(recommendations)}\"\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error during recommendation processing: {e}\")\n",
    "                return \"Sorry, something went wrong while processing your request.\"\n",
    "\n",
    "        # Handle Unknown Questions\n",
    "        return \"I'm really not sure I can help you out with this one. Maybe I could give you some movie recommendations? I've been watching a ton of TV lately.\"\n",
    "\n",
    "    def reset_last_question_info(self):\n",
    "        \"\"\"Resets the last question context.\"\"\"\n",
    "        self.expecting_response = False\n",
    "        \n",
    "        for key, value in self.last_question_info.items():\n",
    "            self.last_question_info[key] = None\n",
    "\n",
    "\n",
    "    ##############################################################################\n",
    "    ##############################################################################\n",
    "    #######################                                #######################\n",
    "    #######################       INDENTIFY QUESTION       #######################\n",
    "    #######################       ------------------       #######################\n",
    "    #######################                                #######################\n",
    "    ##############################################################################\n",
    "    ##############################################################################\n",
    "    \n",
    "    def identify_question(self, question, candidate_labels, threshold=0.35, uncertainty=False):\n",
    "        \n",
    "        try:\n",
    "            result = self.classifier(question, candidate_labels)\n",
    "            top_label = result[\"labels\"][0]\n",
    "            top_score = result[\"scores\"][0]\n",
    "            second_top_label = result[\"labels\"][1]\n",
    "\n",
    "            if not uncertainty:\n",
    "                return top_label\n",
    "\n",
    "            self.last_question_info = {\"question\": question, \"suggested_label\": [top_label, second_top_label]}\n",
    "    \n",
    "            # High confidence\n",
    "            if top_score >= threshold:\n",
    "                return top_label\n",
    "\n",
    "            # Low-confidence scenario: expect clarification\n",
    "            self.expecting_response = True\n",
    "    \n",
    "            return top_label\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error during question identification: {e}\")\n",
    "            return \"Sorry, something went wrong while processing your request.\"\n",
    "\n",
    "\n",
    "\n",
    "    ##############################################################################\n",
    "    ##############################################################################\n",
    "    #######################                                #######################\n",
    "    #######################         EDIT DISTANCE          #######################\n",
    "    #######################         -------------          #######################\n",
    "    #######################                                #######################\n",
    "    ##############################################################################\n",
    "    ##############################################################################\n",
    "\n",
    "    def match_entity_editdistance(self, entity, dictionary, threshold=1):\n",
    "        \"\"\"\n",
    "        Matches the given entity to the closest node in the dictionary based on edit distance.\n",
    "        Returns None if the closest match exceeds the specified distance threshold.\n",
    "        \"\"\"\n",
    "        best_match = None\n",
    "        min_distance = threshold + 1\n",
    "\n",
    "        for key in dictionary.keys():\n",
    "            distance = editdistance.eval(key.lower(), entity.lower())\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                best_match = key\n",
    "                best_match_value = dictionary[key]\n",
    "                best_match_distance = distance\n",
    "        \n",
    "        if min_distance <= threshold:\n",
    "            return best_match, best_match_value, best_match_distance\n",
    "        \n",
    "        return None, None, None\n",
    "\n",
    "    ##############################################################################\n",
    "    ##############################################################################\n",
    "    #######################                                #######################\n",
    "    #######################      EXTRACT KNOWN ENTITY      #######################\n",
    "    #######################      --------------------      #######################\n",
    "    #######################                                #######################\n",
    "    ##############################################################################\n",
    "    ##############################################################################\n",
    "   \n",
    "    def extract_known_entity(self, sentence, dictionary, max_ngram_size=3, threshold=2, confidence=0.6, use_similarity=False):\n",
    "    \n",
    "        try:\n",
    "            # Step 1: Parse the sentence and filter stopwords\n",
    "            doc = nlp(sentence)\n",
    "            meaningful_words = [token.text for token in doc if not token.is_stop and token.is_alpha]\n",
    "    \n",
    "            # Step 2: Generate prioritized n-grams\n",
    "            ngrams = []\n",
    "            for size in range(max_ngram_size, 0, -1):\n",
    "                ngrams += [\" \".join(meaningful_words[i:i+size]) \n",
    "                           for i in range(len(meaningful_words) - size + 1)]\n",
    "    \n",
    "            # Step 3: Try edit distance matching first\n",
    "            for ngram in ngrams:\n",
    "                match, _, _ = self.match_entity_editdistance(ngram, dictionary, threshold=threshold)\n",
    "                if match:\n",
    "                    return match, ngram\n",
    "    \n",
    "            # Step 4: Fallback to similarity matching if edit distance failed\n",
    "            for ngram in ngrams:\n",
    "                matches = self.find_match(ngram, dictionary, confidence=confidence)\n",
    "                if matches:\n",
    "                    return matches[0], ngram\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting known entity: {e}\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def find_match(self, phrase, dictionary, n=5, confidence=0.6):\n",
    "    \n",
    "        phrase_token = nlp(phrase)\n",
    "        similarities = []\n",
    "    \n",
    "        try:\n",
    "            # Calculate similarity between phrase and each value in the dictionary\n",
    "            for key, value in dictionary.items():\n",
    "                key_token = nlp(key)\n",
    "                similarity = phrase_token.similarity(key_token)\n",
    "                if similarity > confidence:\n",
    "                    similarities.append((key, similarity))\n",
    "    \n",
    "            # Sort by similarity in descending order and return top matches\n",
    "            top_matches = sorted(similarities, key=lambda x: x[1], reverse=True)[:n]\n",
    "            return [match[0] for match in top_matches]\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"Error in finding match: {e}\")\n",
    "            return []   \n",
    "\n",
    "    ##############################################################################\n",
    "    ##############################################################################\n",
    "    #######################                                #######################\n",
    "    #######################            MULTIMEDIA          #######################\n",
    "    #######################            ----------          #######################\n",
    "    #######################                                #######################\n",
    "    ##############################################################################\n",
    "    ##############################################################################\n",
    "    \n",
    "    def get_imdb_id(self, entity_name):\n",
    "        \"\"\"\n",
    "        Retrieves the IMDb ID of an entity using graph.value().\n",
    "        Returns None if not found or in case of an error.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            entity_uri = self.ent2uri.get(entity_name)\n",
    "            if not entity_uri:\n",
    "                return None  # No matching URI\n",
    "            \n",
    "            imdb_id = self.graph.value(\n",
    "                subject=rdflib.URIRef(entity_uri), \n",
    "                predicate=self.namespaces['WDT'].P345\n",
    "            )\n",
    "            return str(imdb_id) if imdb_id else None\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving IMDb ID for {entity_name}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_image_from_json(self, imdb_id):\n",
    "\n",
    "        try:\n",
    "            # Step 1: Exact match search (only the IMDb ID in 'cast')\n",
    "            exact_matches = [img['img'] for img in self.images if img['cast'] == [imdb_id]]\n",
    "    \n",
    "            if exact_matches:\n",
    "                return f\"image:{exact_matches[0].replace('.jpg', '')}\"\n",
    "    \n",
    "            # Step 2: General search (IMDb ID in 'cast')\n",
    "            general_matches = [img['img'] for img in self.images if imdb_id in img['cast']]\n",
    "    \n",
    "            if general_matches:\n",
    "                return f\"image:{general_matches[0].replace('.jpg', '')}\"\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving image from JSON: {e}\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "    ##############################################################################\n",
    "    ##############################################################################\n",
    "    #######################                                #######################\n",
    "    #######################                NER             #######################\n",
    "    #######################                ---             #######################\n",
    "    #######################                                #######################\n",
    "    ##############################################################################\n",
    "    ##############################################################################\n",
    "\n",
    "    def NER(self, question, dictionary, max_distance=5):\n",
    "    \n",
    "        # Preprocess the question\n",
    "        cleaned_question = re.sub(r'[:!\\\\-]', '', question)\n",
    "        cleaned_question = re.sub(r'\\s+', ' ', cleaned_question).strip()\n",
    "        question = cleaned_question\n",
    "        \n",
    "        extracted_entities = []\n",
    "    \n",
    "        # Step 1: Use NER to extract entities\n",
    "        try:\n",
    "            entities = self.ner_pipeline(question)\n",
    "        except Exception as e:\n",
    "            print(f\"NER Pipeline Error: {e}\")\n",
    "            return extracted_entities\n",
    "        \n",
    "        # Step 2: Process Extracted Entities\n",
    "        if entities:\n",
    "            # Match entities to graph nodes\n",
    "            for entity in entities[:]:\n",
    "                match_result = self.match_entity_editdistance(entity['word'], dictionary, threshold=max_distance)\n",
    "                \n",
    "                # Handle no match case\n",
    "                if match_result[0] is None:\n",
    "                    entities.remove(entity)\n",
    "                else:\n",
    "                    # Extract and update matched entity\n",
    "                    match_key, match_value, distance = match_result\n",
    "                    entity['word'] = match_key\n",
    "        \n",
    "        return entities\n",
    "\n",
    "\n",
    "    ##############################################################################\n",
    "    ##############################################################################\n",
    "    #######################                                #######################\n",
    "    #######################           EMBEDDINGS           #######################\n",
    "    #######################           ----------           #######################\n",
    "    #######################                                #######################\n",
    "    ##############################################################################\n",
    "    ##############################################################################\n",
    "    \n",
    "    def extract_entities_and_predicates(self, question, dictionary):\n",
    "\n",
    "        # Step 1: Extract Predicates\n",
    "        extracted_predicate, matched_ngram = self.extract_known_entity(\n",
    "            sentence=question, \n",
    "            dictionary=self.pred2uri, \n",
    "            use_similarity=True\n",
    "        )\n",
    "    \n",
    "        if matched_ngram and extracted_predicate:\n",
    "        \n",
    "            # Step 2: Lowercase the Matched Predicate N-gram from the Question to improve NER\n",
    "            question = re.sub(\n",
    "                r'\\b' + re.escape(matched_ngram) + r'\\b', \n",
    "                matched_ngram.lower(), \n",
    "                question, \n",
    "                flags=re.IGNORECASE\n",
    "            ).strip()\n",
    "        \n",
    "        # Step 3: Extract Entities Using NER\n",
    "        entities = self.NER(question, dictionary)\n",
    "        return extracted_predicate, entities\n",
    "\n",
    "    def compute_embedding_answer(self, extracted_predicate, extracted_entity):\n",
    "    \n",
    "        # Step 4: Convert Entities and Predicates to Embeddings\n",
    "        entity_embedding = self.extract_embedding(extracted_entity, 'entity')\n",
    "        predicate_embedding = self.extract_embedding(extracted_predicate, 'predicate')\n",
    "    \n",
    "        # Ensure all embeddings were found\n",
    "        if entity_embedding is None or predicate_embedding is None:\n",
    "            return []\n",
    "    \n",
    "        # Step 5: Compute Answer Using Embeddings\n",
    "        query_embedding = entity_embedding + predicate_embedding\n",
    "        answer = self.find_similarities(query_embedding, n=3)\n",
    "    \n",
    "        return answer if answer else []\n",
    "\n",
    "\n",
    "    def find_similarities(self, embedding, n=5):\n",
    "        \"\"\"\n",
    "        Finds the top-n most similar entities based on embedding distance.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            embedding = np.atleast_2d(embedding)\n",
    "            distances = pairwise_distances(embedding, self.entity_matrix)\n",
    "            similar_entities = [\n",
    "                self.nodes[self.id2ent[idx]]\n",
    "                for idx in distances.argsort().reshape(-1)[:n]\n",
    "            ]\n",
    "            return similar_entities\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error finding similar entities: {e}\")\n",
    "            return []\n",
    "\n",
    "    \n",
    "    def extract_embedding(self, label, embedding_type='entity'):\n",
    "        \"\"\"\n",
    "        Extracts the embedding vector for a given label (entity or predicate).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if embedding_type == 'entity':\n",
    "                uri_dict, id_dict, embed_matrix = self.ent2uri, self.ent2id, self.entity_matrix\n",
    "            else:\n",
    "                uri_dict, id_dict, embed_matrix = self.pred2uri, self.pred2id, self.predicate_matrix\n",
    "    \n",
    "            # Convert label to URI and lookup ID\n",
    "            uri = uri_dict[label]\n",
    "            row_id = id_dict[uri]\n",
    "    \n",
    "            # Extract embedding from matrix\n",
    "            embedding = embed_matrix[row_id]\n",
    "            return embedding\n",
    "        \n",
    "        except KeyError:\n",
    "            print(f\"Label '{label}' not found in {embedding_type} embeddings.\")\n",
    "            return None\n",
    "\n",
    "    def extract_label(self, embedding, embedding_type='entity'):\n",
    "        try:\n",
    "            if embedding_type == 'entity':\n",
    "                embed_matrix, id_dict, label_dict = self.entity_matrix, self.id2ent, self.nodes\n",
    "            else:\n",
    "                embed_matrix, id_dict, label_dict = self.predicate_matrix, self.id2pred, self.predicates\n",
    "    \n",
    "            # Find the closest embedding in the matrix\n",
    "            idx = np.where((embed_matrix == embedding).all(axis=1))[0][0]\n",
    "            uri = id_dict[idx]\n",
    "            label = label_dict[uri]\n",
    "            return label\n",
    "        \n",
    "        except IndexError:\n",
    "            print(\"Embedding not found in matrix.\")\n",
    "            return None\n",
    "        \n",
    "    ##############################################################################\n",
    "    ##############################################################################\n",
    "    #######################                                #######################\n",
    "    #######################        FACTUAL QUESTIONS       #######################\n",
    "    #######################        -----------------       #######################\n",
    "    #######################                                #######################\n",
    "    ##############################################################################\n",
    "    ##############################################################################\n",
    "\n",
    "    def match_question_pattern(self, question):\n",
    "        for pattern, qtype, requires_matching in self.question_patterns:\n",
    "            match = re.search(pattern, question, re.IGNORECASE)\n",
    "            if match:\n",
    "                params = match.groupdict()\n",
    "                params['type'] = qtype\n",
    "                params['requires_matching'] = requires_matching\n",
    "                return params\n",
    "        return None\n",
    "\n",
    "    def generate_sparql_query(self, params):\n",
    "        qtype = params.get('type')\n",
    "\n",
    "        if qtype == 'who':\n",
    "            return f\"\"\"\n",
    "            SELECT ?result WHERE {{\n",
    "                ?entity rdfs:label \"{params['entity']}\"@en .\n",
    "                ?entity <{self.pred2uri[params['relation']]}> ?item .\n",
    "                ?item rdfs:label ?result .\n",
    "                FILTER (lang(?result) = 'en')\n",
    "            }}\n",
    "            \"\"\"\n",
    "\n",
    "        if qtype == 'find_word_in_title':\n",
    "            return f\"\"\"\n",
    "            SELECT ?movieLabel WHERE {{\n",
    "                ?movie rdfs:label ?movieLabel .\n",
    "                FILTER(CONTAINS(LCASE(?movieLabel), LCASE(\"{params['word']}\"))) .\n",
    "                FILTER (lang(?movieLabel) = 'en')\n",
    "                LIMIT 5\n",
    "            }}\n",
    "            \"\"\"\n",
    "\n",
    "        if qtype == 'movies_rating_above':\n",
    "            return f\"\"\"\n",
    "            SELECT ?movieLabel WHERE {{\n",
    "                ?movie ddis:rating ?rating .\n",
    "                FILTER(?rating > {params['number']}) .\n",
    "                ?movie rdfs:label ?movieLabel .\n",
    "                FILTER (lang(?movieLabel) = 'en')\n",
    "            }} ORDER BY DESC(?rating) LIMIT 1\n",
    "            \"\"\"\n",
    "\n",
    "        if qtype == 'movies_rating_below':\n",
    "            return f\"\"\"\n",
    "            SELECT ?movieLabel WHERE {{\n",
    "                ?movie ddis:rating ?rating .\n",
    "                FILTER(?rating < {params['number']}) .\n",
    "                ?movie rdfs:label ?movieLabel .\n",
    "                FILTER (lang(?movieLabel) = 'en')\n",
    "            }} ORDER BY DESC(?rating)\n",
    "            \"\"\"\n",
    "\n",
    "        if qtype == 'entity_first_alphabetically':\n",
    "            return f\"\"\"\n",
    "            SELECT ?entity_label WHERE {{\n",
    "                ?entity wdt:P31 <{self.ent2uri[params['entity']]}> .\n",
    "                ?entity rdfs:label ?entity_label .\n",
    "                FILTER (lang(?entity_label) = 'en')\n",
    "            }} ORDER BY ASC(?entity_label)\n",
    "            \"\"\"\n",
    "\n",
    "        if qtype == 'entity_last_alphabetically':\n",
    "            return f\"\"\"\n",
    "            SELECT ?entity_label WHERE {{\n",
    "                ?entity wdt:P31 <{self.ent2uri[params['entity']]}> .\n",
    "                ?entity rdfs:label ?entity_label .\n",
    "                FILTER (lang(?entity_label) = 'en')\n",
    "            }} ORDER BY DESC(?entity_label)\n",
    "            \"\"\"\n",
    "            \n",
    "        elif qtype == 'release_date':\n",
    "            return f\"\"\"\n",
    "            PREFIX ddis: <http://ddis.ch/atai/>  PREFIX wd: <http://www.wikidata.org/entity/>  \n",
    "            PREFIX wdt: <http://www.wikidata.org/prop/direct/>  PREFIX schema: <http://schema.org/>\n",
    "            SELECT ?releaseDate WHERE {{\n",
    "                ?movie rdfs:label \"{params['entity']}\"@en .\n",
    "                ?movie wdt:P577 ?releaseDate .\n",
    "            }} ORDER BY ASC(?releaseDate) LIMIT 1\n",
    "            \"\"\"\n",
    "\n",
    "        return None\n",
    "\n",
    "    def query_graph(self, sparql_query):\n",
    "        try:\n",
    "            qres = self.graph.query(sparql_query)\n",
    "            return [str(row[0]) for row in qres] if qres else []\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] SPARQL Query Execution Error: {e}\")\n",
    "            return []\n",
    "\n",
    "    ##############################################################################\n",
    "    ##############################################################################\n",
    "    #######################                                #######################\n",
    "    #######################        RECOMMENDATIONS         #######################\n",
    "    #######################        ---------------         #######################\n",
    "    #######################                                #######################\n",
    "    ##############################################################################\n",
    "    ##############################################################################\n",
    "\n",
    "\n",
    "    def compute_recommendation(self, liked_movies, n=5):\n",
    "\n",
    "        try:\n",
    "            # Extract embeddings for liked movies\n",
    "            liked_movies_embeddings = []\n",
    "            for movie in liked_movies:\n",
    "                # Extract embedding directly if the movie is known\n",
    "                embedding = self.extract_embedding(movie, 'entity')\n",
    "                if embedding is not None:\n",
    "                    liked_movies_embeddings.append(embedding)\n",
    "        \n",
    "            # Return early if no embeddings found\n",
    "            if not liked_movies_embeddings:\n",
    "                return []\n",
    "    \n",
    "            # Average embedding calculation\n",
    "            avg_embedding = np.mean(liked_movies_embeddings, axis=0)\n",
    "    \n",
    "            # Compute similarity with all movie embeddings\n",
    "            similarities = cosine_similarity(avg_embedding.reshape(1, -1), self.entity_matrix)\n",
    "    \n",
    "            # Sort and filter recommendations\n",
    "            sorted_indices = similarities.argsort()[0][::-1]\n",
    "            excluded_movies = set(liked_movies)\n",
    "    \n",
    "            recommended_movies = []\n",
    "            for i in sorted_indices:\n",
    "                label = self.extract_label(self.entity_matrix[i], 'entity')\n",
    "                if label and label not in excluded_movies:\n",
    "                    recommended_movies.append(label)\n",
    "                    if len(recommended_movies) >= n:\n",
    "                        break\n",
    "    \n",
    "            return recommended_movies[:n]\n",
    "    \n",
    "        except Exception as e:\n",
    "            return []\n",
    "\n",
    "    ##############################################################################\n",
    "    ##############################################################################\n",
    "    #######################                                #######################\n",
    "    #######################          CROWDSOURCING         #######################\n",
    "    #######################          -------------         #######################\n",
    "    #######################                                #######################\n",
    "    ##############################################################################\n",
    "    ##############################################################################\n",
    "\n",
    "    def crowdsource_search_with_valid_workers(self, entity, relation):\n",
    "\n",
    "        try:\n",
    "            if not isinstance(entity, str) or not isinstance(relation, str):\n",
    "                raise ValueError(f\"Entity and relation must be strings, got {type(entity)} and {type(relation)}.\")\n",
    "            \n",
    "            res = self.valid_workers[\n",
    "                (self.valid_workers['Input1ID'].astype(str) == entity) & \n",
    "                (self.valid_workers['Input2ID'].astype(str) == relation)\n",
    "            ]\n",
    "    \n",
    "            if res.empty:\n",
    "                return None\n",
    "    \n",
    "            hit_type_id = res['HITTypeId'].iloc[0]\n",
    "            kappa = self.fleiss_kappa_per_batch.get(hit_type_id, 0)\n",
    "            kappa = round(kappa, 3)\n",
    "    \n",
    "            final_answer, correct_count, incorrect_count = self.majority_vote(res)\n",
    "    \n",
    "            answer_column = 'Input3ID' if final_answer == \"CORRECT\" else 'FixValue'\n",
    "            answers = res[answer_column].dropna().unique()\n",
    "    \n",
    "            converted_answers = [\n",
    "                self.get_entity_label(ans) if ans.startswith(\"wd:\") else ans for ans in answers\n",
    "            ]\n",
    "            if len(converted_answers) == 1:\n",
    "                answers_str = converted_answers[0]\n",
    "                return (f\"The answer is {answers_str}. \"\n",
    "                    f\"[Crowd, inter-rater agreement: {kappa}, \"\n",
    "                    f\"The answer distribution for this task was {correct_count} support votes, \"\n",
    "                    f\"{incorrect_count} reject votes]\")\n",
    "            elif len(converted_answers) == 2:\n",
    "                answers_str = ' and '.join(converted_answers)\n",
    "            else:\n",
    "                answers_str = ', '.join(converted_answers[:-1]) + f\", and {converted_answers[-1]}\"\n",
    "            \n",
    "            return (f\"The answers are {answers_str}. \"\n",
    "                    f\"[Crowd, inter-rater agreement: {kappa}, \"\n",
    "                    f\"The answer distribution for this task was {correct_count} support votes, \"\n",
    "                    f\"{incorrect_count} reject votes]\")\n",
    "\n",
    "    \n",
    "        except KeyError as e:\n",
    "            return None\n",
    "    \n",
    "        except Exception as e:\n",
    "            return None\n",
    "\n",
    "\n",
    "    def find_entity_in_crowd_data(self, extracted_entity_label, entity='entity'):\n",
    "\n",
    "        try:\n",
    "                \n",
    "            dictionary = self.ent2uri if entity == 'entity' else self.pred2uri\n",
    "            \n",
    "            match = self.match_entity_editdistance(extracted_entity_label, dictionary)\n",
    "            \n",
    "            if match:\n",
    "                matched_node, matched_id, _ = match\n",
    "                if entity == 'entity':\n",
    "                    stripped_id = \"wd:\" + matched_id.split('/')[-1]\n",
    "                    result = self.crowd_data[self.crowd_data['Input1ID'] == stripped_id]\n",
    "                    if not result.empty:\n",
    "                        id = result['Input1ID'].iloc[0]\n",
    "                else:\n",
    "                    stripped_id = \"wdt:\" + matched_id.split('/')[-1]\n",
    "                    result = self.crowd_data[self.crowd_data['Input2ID'] == stripped_id]\n",
    "                    if not result.empty:\n",
    "                        id = result['Input2ID'].iloc[0]\n",
    "\n",
    "                return id\n",
    "        \n",
    "            # Entity not found\n",
    "            return None\n",
    "    \n",
    "        except KeyError as e:\n",
    "            return None\n",
    "    \n",
    "        except Exception as e:\n",
    "            return None\n",
    "\n",
    "    def majority_vote(self, res):\n",
    "        \n",
    "        try:\n",
    "            correct_count = (res['AnswerLabel'] == 'CORRECT').sum()\n",
    "            incorrect_count = (res['AnswerLabel'] == 'INCORRECT').sum()\n",
    "    \n",
    "            if correct_count == 0 and incorrect_count == 0:\n",
    "                raise ValueError(\"No valid votes found in the result set.\")\n",
    "    \n",
    "            final_answer = \"CORRECT\" if correct_count >= incorrect_count else \"INCORRECT\"\n",
    "            return final_answer, correct_count, incorrect_count\n",
    "    \n",
    "        except Exception as e:\n",
    "            return \"INCORRECT\", 0, 0\n",
    "\n",
    "\n",
    "    \n",
    "    def get_entity_label(self, entity_id):\n",
    "\n",
    "        try:\n",
    "            graph = Graph()\n",
    "            url = f\"https://www.wikidata.org/wiki/Special:EntityData/{entity_id[3:]}.ttl\"\n",
    "            graph.parse(url, format=\"turtle\")\n",
    "    \n",
    "            entity_uri = URIRef(f\"http://www.wikidata.org/entity/{entity_id[3:]}\")\n",
    "            for label in graph.objects(entity_uri, URIRef(\"http://schema.org/name\")):\n",
    "                return label.value\n",
    "    \n",
    "        except Exception as e:\n",
    "            return \"Label not found\"\n",
    "\n",
    "    ##############################################################################\n",
    "    ##############################################################################\n",
    "    #######################                                #######################\n",
    "    #######################           PARAPHRASING         #######################\n",
    "    #######################           ------------         #######################\n",
    "    #######################                                #######################\n",
    "    ##############################################################################\n",
    "    ##############################################################################\n",
    "    \n",
    "    def paraphrase(self, context: str, num_return_sequences: int = 5, num_beams: int = 5):\n",
    "\n",
    "        try:\n",
    "            # Tokenize the input\n",
    "            batch = self.tokenizer(\n",
    "                [context], truncation=True, padding='longest',\n",
    "                max_length=60, return_tensors=\"pt\"\n",
    "            )\n",
    "    \n",
    "            # Generate paraphrases\n",
    "            translated = self.paraphrase_model.generate(\n",
    "                **batch, max_length=60, num_beams=num_beams, \n",
    "                num_return_sequences=num_return_sequences, temperature=1.5\n",
    "            )\n",
    "    \n",
    "            # Decode and remove final period if present\n",
    "            tgt_text = self.tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "            if tgt_text:\n",
    "                result = random.choice(tgt_text).strip()\n",
    "                return result[:-1] if result.endswith('.') else result\n",
    "    \n",
    "            return \"I couldn't find a paraphrase.\"\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"Paraphrasing Error: {e}\")\n",
    "            return \"Something went wrong while paraphrasing. Please try again later.\"\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b7edbc-3650-403e-b7ab-cc96914efe90",
   "metadata": {},
   "source": [
    "# SpeakEasy Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dda4406-df69-447c-80d9-6f00a6d1c375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login successful. Session token: 9xnp3Q78utK_I48li4z1Bhpa3qgYeYgM\n",
      "\t- Chatroom fc1b5fa6-95db-460f-ab26-789cb4d03f36 - new message #0: 'who is the director of Inception' - 23:00:00, 09-12-2024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at tuner007/pegasus_paraphrase and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Chatroom fc1b5fa6-95db-460f-ab26-789cb4d03f36 - new message #3: 'can you recommend some movies since I like The Great Gatsby, Inception and The Godfather' - 23:00:17, 09-12-2024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at tuner007/pegasus_paraphrase and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from speakeasypy import Speakeasy, Chatroom\n",
    "from typing import List\n",
    "import time\n",
    "\n",
    "DEFAULT_HOST_URL = 'https://speakeasy.ifi.uzh.ch'\n",
    "listen_freq = 2\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, username, password):\n",
    "        self.username = username\n",
    "        # Initialize the Speakeasy Python framework and login.\n",
    "        self.speakeasy = Speakeasy(host=DEFAULT_HOST_URL, username=username, password=password)\n",
    "        self.speakeasy.login()  # This framework will help you log out automatically when the program terminates.\n",
    "\n",
    "    def listen(self):\n",
    "        graph = rdflib.Graph()\n",
    "        graph.parse('/Users/gianmarcoalbano/Desktop/Advanced topics in AI/Speakeasy Project/Datasets/14_graph.nt', format='turtle')\n",
    "        while True:\n",
    "            # only check active chatrooms (i.e., remaining_time > 0) if active=True.\n",
    "            rooms: List[Chatroom] = self.speakeasy.get_rooms(active=True)\n",
    "            for room in rooms:\n",
    "                if not room.initiated:\n",
    "                    # send a welcome message if room is not initiated\n",
    "                    room.post_messages(f'Hello! This is a welcome message from {room.my_alias}.')\n",
    "                    room.initiated = True\n",
    "                # Retrieve messages from this chat room.\n",
    "                # If only_partner=True, it filters out messages sent by the current bot.\n",
    "                # If only_new=True, it filters out messages that have already been marked as processed.\n",
    "                for message in room.get_messages(only_partner=True, only_new=True):\n",
    "                    print(\n",
    "                        f\"\\t- Chatroom {room.room_id} \"\n",
    "                        f\"- new message #{message.ordinal}: '{message.message}' \"\n",
    "                        f\"- {self.get_time()}\")\n",
    "\n",
    "                    # Implement your agent here #\n",
    "                    bot = Chatbot(graph, dictionaries, images, crowdsourcing_files)\n",
    "                    result = bot.answer_question(message.message)\n",
    "\n",
    "                    if message.message == \"Show me a picture of Abu\":\n",
    "                        result = \"Sorry but that sexy beast is not in the database\"\n",
    "\n",
    "                    # Send a message to the corresponding chat room using the post_messages method of the room object.\n",
    "                    room.post_messages(f\"'{result}' \")\n",
    "                    # Mark the message as processed, so it will be filtered out when retrieving new messages.\n",
    "                    room.mark_as_processed(message)\n",
    "\n",
    "                # Retrieve reactions from this chat room.\n",
    "                # If only_new=True, it filters out reactions that have already been marked as processed.\n",
    "                for reaction in room.get_reactions(only_new=True):\n",
    "                    print(\n",
    "                        f\"\\t- Chatroom {room.room_id} \"\n",
    "                        f\"- new reaction #{reaction.message_ordinal}: '{reaction.type}' \"\n",
    "                        f\"- {self.get_time()}\")\n",
    "\n",
    "                    # Implement your agent here #\n",
    "\n",
    "                    room.post_messages(f\"Received your reaction: '{reaction.type}' \")\n",
    "                    room.mark_as_processed(reaction)\n",
    "\n",
    "            time.sleep(listen_freq)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_time():\n",
    "        return time.strftime(\"%H:%M:%S, %d-%m-%Y\", time.localtime())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    demo_bot = Agent(\"swift-comet\", \"X2wqU6D3\")\n",
    "    demo_bot.listen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adecc40-e2c0-4c6c-ba69-bbfa71a16fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
